\documentclass[12pt]{article}

\usepackage{color}
\usepackage{times}
\usepackage{fullpage}
\usepackage{hyperref}

%\usepackage{draftwatermark}

\newcommand{\akey}[1]{\textbf{#1}}

\let\OLDthebibliography\thebibliography
\renewcommand\thebibliography[1]{
  \OLDthebibliography{#1}
  \setlength{\parskip}{2pt}
  \setlength{\itemsep}{1.5pt plus 0.3ex}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\date{\small This version revised \the\year-\the\month-\the\day}

\title{10th International Satisfiability Modulo Theories Competition
  (SMT-COMP 2015): Rules and Procedures}

% [morgan] do our own layout of authors; the four-author layout spacing
% was screwed up...
\def\doauthor#1{{%
  \hsize.5\hsize \advance\hsize by-1cm %
  \def\\{\hss\egroup\hbox to\hsize\bgroup\strut\hss}%
  \vbox{\hbox to\hsize\bgroup\strut\hss#1\hss\egroup}}}%

\def\header#1{\medskip\noindent\textbf{#1}}

\author{%
Sylvain Conchon \\
Paris-Sud University \\
France \\
{\small\href{mailto:Sylvain.Conchon@lri.fr}{\tt Sylvain.Conchon@lri.fr}} \\
\and
David D{\'e}harbe \\
Federal University of Rio Grande do Norte \\
Brazil \\
{\small\href{mailto:David.Deharbe@loria.fr}{\tt David.Deharbe@loria.fr}} \\
\and
Tjark Weber \\
Uppsala University \\
Sweden \\
{\small\href{mailto:tjark.weber@it.uu.se}{\tt tjark.weber@it.uu.se}} \\
\\
}

\maketitle

\noindent Comments on this document should be emailed to the SMT-COMP
mailing list (see below) or, if necessary, directly to the organizers.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Communication}

Interested parties should subscribe to the SMT-COMP mailing list.
Important late-breaking news and any necessary clarifications and
edits to these rules will be announced there, and it is the primary
way that such announcements will be communicated.

\begin{itemize}
\item SMT-COMP mailing list: \href{mailto:smt-comp@cs.nyu.edu}{\tt
  smt-comp@cs.nyu.edu}
\item Sign-up site for the mailing list:
  \url{cs.nyu.edu/mailman/listinfo/smt-comp}
\end{itemize}

\noindent Additional material will be made available at the
competition web site, \url{www.smtcomp.org} or
\url{smtcomp.sourceforge.net/2015}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage

\section{Important Dates}
\label{sec:important}

\begin{description}
\item[May~1] Deadline for new benchmark contributions.
\item[June~1] Deadline for first versions of solvers (for all tracks),
  including information about which tracks and divisions are being
  entered, and magic numbers for benchmark scrambling.
\item[June~7] Final versions of competition tools (e.g., benchmark
  scrambler) are made available.  Benchmark libraries are frozen.
\item[June~14] Deadline for final versions of solvers, including
  system descriptions.
\item[June~15] Opening value of NYSE Composite Index used to complete
  random seed for benchmark scrambling.
\item[July~18/19] SMT Workshop; end of competition, presentation of
  results.
\end{description}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}

The annual Satisfiability Modulo Theories Competition~(SMT-COMP) is
held to spur advances in SMT solver implementations on benchmark
formulas of practical interest.  Public competitions are a well-known
means of stimulating advancement in software tools.  For example, in
automated reasoning, the CASC and SAT competitions for first-order and
propositional reasoning tools, respectively, have spurred significant
innovation in their fields~\cite{leberre+03,PSS02}.  More information
on the history and motivation for SMT-COMP can be found at the
competition web site, \url{www.smtcomp.org}, and in reports on
previous
competitions~(\cite{SMTCOMP-JAR,SMTCOMP-FMSD,BDOS08,SMTCOMP-2008,SMTCOMP-2014,SMTCOMP-2012,SMTEVAL-2013}).

SMT-COMP~2015 is affiliated with the SMT Workshop~2015
(\url{http://smt2015.csl.sri.com/}), which is associated with CAV~2015
(\url{http://i-cav.org/2015/}).  The SMT Workshop and the main CAV
conference will include a block of time to present the competitors and
results of the competition.

Accordingly, researchers are highly encouraged to submit both new
benchmarks and new or improved solvers to raise the level of
competition and advance the state-of-the-art in automated SMT problem
solving.

SMT-COMP 2015 will have two tracks: the conventional main track and an
application (i.e., incremental) track.  Within each track there are
multiple divisions, where each division uses benchmarks from a
specific SMT-LIB logic (or group of logics).  We will recognize
winners as measured by number of benchmarks solved; we will also
recognize solvers based on additional criteria.

The rest of this document, revised from the previous
version,\footnote{Earlier versions of this document include
  contributions from Clark Barrett, Roberto Bruttomesso, David Cok,
  David D{\'e}harbe, Morgan Deters, Alberto Griggio, Albert Oliveras,
  Aaron Stump, and Tjark Weber.} describes the rules and competition
procedures for SMT-COMP~2015.  The principal changes from the previous
competition rules are the following:
\begin{itemize}
\item All solvers will be run with $n=4$ cores available.  We will
  recognize both best sequential and best parallel performance in all
  main track divisions, using different CPU time limits.
\item The division scoring will take time spent on unsolved benchmarks
  into account.  Previous competitions only considered time spent on
  solved benchmarks.  However, the number of solved benchmarks still
  takes precedence over run-time.
\item In addition to recognizing the best solver in each division, we
  will recognize solvers that perform best according to
  competition-wide criteria, emphasizing breadth of supported logics.
  These criteria will be a refinement of the criteria used for the
  FLoC Olympic Games medals in~2014.
\item We plan to include new divisions for floating-point arithmetic.
  These should be considered experimental in~2015, and will not be
  included in the competition-wide ranking.
\item We plan to run solvers on all eligible benchmarks.  Previous
  competitions used a proper subset of eligible benchmarks, causing
  competition results to be affected by a random selection process.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Entrants}
\label{sec:entrants}

\header{Solver submission.}
%
An entrant to SMT-COMP is an SMT solver submitted by its authors using
the StarExec (\url{http://www.starexec.org}) service.  The execution
service enables members of the SMT research community to run solvers
on jobs consisting of benchmarks from the SMT-LIB benchmark library.
Jobs are run on a shared computer cluster.  The execution service is
provided free of charge, but it does require registration to create a
login account.  Registered users may then upload their own solvers to
run, or may run public solvers already uploaded to the service.

For participation in SMT-COMP, a solver must be uploaded to StarExec,
made publicly available, and the organizers informed of its presence
{\em and the tracks and divisions in which it is
  participating}. StarExec supports solver configurations; for
clarity, each submitted solver must have one configuration only.  A
submission must also include a 32-bit unsigned integer.  These integer
numbers, collected from all submissions, are used to seed the
benchmark scrambler.

Information about how to configure and upload a solver is contained in
the StarExec user guide,
\url{https://wiki.uiowa.edu/display/stardev/User+Guide}.

\header{System description.}
%
As part of a submission, SMT-COMP entrants are encouraged to provide a
short (1--2 pages) description of the system.  This should include a
list of all authors of the system, their present institutional
affiliations, and any appropriate acknowledgements.  The programming
language(s) and basic SMT solving approach employed should be
described (e.g., lazy integration of a Nelson-Oppen combination with
SAT, translation to SAT, etc.).  System descriptions are encouraged to
include a URL for a web site for the submitted tool.  System
descriptions may be submitted after the solver deadline, but to be
useful should be sent to the organizers before the competition ends.
We intend to make system descriptions publicly available.

\header{Multiple versions.}
%
The organizers' intent is to promote as wide a comparison among
solvers and solver options as possible.  However, if the number of
solver submissions is too large for the computational resources
available to the competition, the organizers reserve the right not to
accept multiple versions of solvers from the same solver team.

\header{Other solvers.}
%
The organizers reserve the right to include other solvers of interest
(such as entrants in previous SMT competitions) in the competition,
e.g., for comparison purposes.

\header{Wrapper tools.}
%
A \emph{wrapper tool} is defined as any solver which calls one or more
SMT solvers not written by the author of the wrapper tool.  The other
solvers are called the \emph{wrapped} solvers.  A wrapper tool must
explicitly acknowledge any solvers that it wraps.  Its system
description should make clear the technical innovations by which the
wrapper tool expects to improve on the wrapped solvers.

\header{Attendance.}
%
Submitters of an SMT-COMP entrant need not be physically present at
the competition or the SMT Workshop to participate or win.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection*{Deadlines}

SMT-COMP entrants must be submitted via StarExec (solvers) \emph{and}
email to the organizers (accompanying information) until the end of
{\bf June~1, 2015} anywhere on earth.  After this date no new entrants
will be accepted.  However, updates to existing entrants on StarExec
will be accepted until the end of {\bf June~14, 2015} anywhere on
earth.

We strongly encourage participants to use this grace period
\emph{only} for the purpose of fixing any bugs that may be discovered,
and not for adding new features, as there may be no opportunity to do
extensive testing using StarExec after the initial deadline.

The solver versions that are present on StarExec at the conclusion of
the grace period will be the ones used for the competition.  Versions
submitted after this time will not be used.  The organizers reserve
the right to start the competition itself at any time after the open
of the New York Stock Exchange on the day after the final solver
deadline.

These deadlines and procedures apply equally to all tracks of the
competition.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Execution of Solvers}

Solvers will be publicly evaluated in all tracks and divisions into
which they have been entered.  All results of the competition will be
made public.

\subsection{Logistics}
\label{sec:logistics}

\header{Dates of competition.}
%
The bulk of the computation will take place during the weeks leading
up to SMT~2015, from about June~15 to July~17.  Intermediate results
will be regularly posted to the SMT-COMP website as the competition
runs.

The organizers reserve the right to prioritize certain competition
tracks or divisions to ensure their timely completion, and in
exceptional circumstances to complete divisions after the SMT
Workshop.

\header{Input and output.}
%
In the main track, a participating solver must read a single benchmark
script, whose name is presented as the solver's first argument.  In
the application track, a trace executor will send commands from a
benchmark script to the solver's standard input channel.

The benchmark script is in the concrete syntax of the SMT-LIB format,
version~2.0, though with a restricted set of commands.  A benchmark
script is a text file containing a sequence of SMT-LIB~v2 commands in
which:
%
\begin{enumerate}
\item In the main track, there may be a (single) \akey{set-option
  :print-success ...} command.  Note that {\tt success} outputs are
  ignored by the post-processor used by the
  competition.\footnote{SMT-LIB~2.0 requires that a solver produce a
    \texttt{success} answer after each \akey{set-logic},
    \akey{declare-sort}, \akey{declare-fun} and \akey{assert} command
    (among others), unless the option \akey{:print-success} is set to
    false.  Ignoring the \texttt{success} outputs allows for
    submitting fully SMT-LIB~2.0 compliant solvers without the need
    for a wrapper script, while still allowing entrants of previous
    competitions to run without changes.}
\item In the application track, the \akey{:print-success} option may
  not be disabled; the application track executor will send an initial
  $\akey{set-option :print-success true}$ command to the solver.
\item The (single) $\akey{set-logic}$ command setting the benchmark's
  logic is the first command after any $\akey{set-option}$ commands.
\item The script next optionally contains (any number of)
  $\akey{set-info}$ commands, which may be ignored by the
  solver. Benchmarks will likely have some of the following
  \akey{set-info} commands: \akey{(set-info :smt-lib-version 2.0)},
  \akey{(set-info :category ...)}, \akey{(set-info :source ...)}, and
  \akey{(set-info :status ...)}.
\item In the main track, there is exactly one $\akey{check-sat}$
  command, following possibly several $\akey{assert}$ commands.
\item In the application track, there are one or more \akey{check-sat}
  commands, each preceded by one or more \akey{assert} commands; there
  may also be zero or more \akey{push 1} commands, and zero or more
  \akey{pop 1} commands, consistent with the uses of those commands in
  the SMT-LIB standard. Each \akey{check-sat} command may be preceded
  by a \akey{set-info :status} command that indicates the expected
  result of the \akey{check-sat} command.
\item The $\akey{exit}$ command is the last command.
\item The formulas in the script belong to the benchmark's logic, with
  any free symbols declared in the script.
\item All sorts declared with a $\akey{declare-sort}$ command must
  have zero arity.
\item Application track scripts may have $\akey{define-sort}$
  commands.
\item No other commands besides the ones just mentioned may be used.
\item Named formulas are not used in benchmark scripts.
\end{enumerate}
%
The SMT-LIB format specification is available from the ``Documents''
section of the SMT-LIB website~\cite{SMT-LIB}.  Solvers will be given
formulas only from the divisions into which they have been entered.

\header{Time and memory limits.}
%
Each SMT-COMP solver will be executed on a dedicated processor of a
competition machine, for each given benchmark, up to a fixed
wall-clock time limit~$T$.  Each processor has 4 cores.  Detailed
machine specifications are available on the competition web site.

The time limit~$T$ is yet to be determined, but it is anticipated to
be 40~minutes of wall-clock time per solver/benchmark
pair.\footnote{The time limit may be adjusted once we know the number
  of competition entrants and eligible benchmarks.}  Solvers that take
more than this time limit will be killed.  Solvers are allowed to
spawn other processes; these will be killed at approximately the same
time as the first started process.

The StarExec service also limits the memory consumption of the solver
processes.  We expect the memory limit per solver/benchmark pair to be
on the order of 60\,GB.  The values of both the time limit and the
memory limit are available to a solver process through environment
variables.  See the StarExec user guide for more information.

\subsection{Main track}
\label{sec:exec:main}

The main track competition will consist of selected benchmarks in each
of the logic divisions.  Each benchmark script will be presented to
the solver as its first command-line argument.  Each SMT-COMP entrant
is then expected to attempt to report on its standard output channel
whether the formula is satisfiable (\texttt{sat}, in lowercase) or
unsatisfiable (\texttt{unsat}).  An entrant may also report
\texttt{unknown} to indicate that it cannot determine satisfiability
of the formula.

The main track competition uses a StarExec postprocessor (named ``SMT2
results conserv") to accumulate the results.

\header{Aborts and unparsable output.}
%
Any \texttt{success} outputs will be ignored.  Solvers that exit
before the time limit without reporting a result (e.g., due to
exhausting memory or crashing) and do not produce output that includes
\texttt{sat}, \texttt{unsat} or \texttt{unknown} will be considered to
have aborted.

\header{Persistent state.}
%
Solvers may create and write to files and directories during the
course of an execution, but they must not read such files back during
later executions.  Each solver is executed with a temporary directory
as its current working directory.  Any generated files should be
produced there (and not, say, in the system's {\tt /tmp} directory).
The StarExec system sets a limit on the amount of disk storage
permitted---typically 20\,GB.  See the StarExec user guide for more
information.  The temporary directory is deleted after the job is
complete.  Solvers must not attempt to communicate with other
machines, e.g., over the network.

\subsection{Application track}

The application track evaluates SMT solvers when interacting with an
external verification framework, e.g., a model checker. This
interaction, ideally, happens by means of an online communication
between the model checker and the solver: the model checker repeatedly
sends queries to the SMT solver, which in turn answers either
\texttt{sat} or \texttt{unsat}.  In this interaction an SMT-solver is
required to accept queries incrementally via its {\em standard input
  channel}.

In order to facilitate the evaluation of the solvers in this track, we
will set up a ``simulation'' of the aforementioned interaction. In
particular each benchmark in the application track represents a
realistic communication trace, containing multiple \akey{check-sat}
commands (possibly with corresponding \akey{push 1}/\akey{pop 1}
commands), which is parsed by a \emph{trace executor}. The trace
executor serves the following purposes:
\begin{itemize}
\item it simulates the online interaction by sending single queries to
  the SMT solver (through stdin);
\item it prevents ``look-ahead'' behaviors of SMT solvers;
\item it records time and answers for each call;
\item it guarantees a fair execution for all solvers by abstracting
  from any possible crash, misbehavior, etc. that may happen on the
  model checker side.
\end{itemize}
%
The trace executor terminates processing the benchmark script upon
receiving an incorrect response from the solver.

The disk space and memory limits for the application track are the
same as for the main track (see Section~\ref{sec:exec:main}).

\header{Input and output.}
%
Participating solvers will be connected to a trace executor, which
will incrementally send commands to the standard input channel of the
solver and read responses from the standard output channel of the
solver.  The commands will be taken from an SMT-LIB~2.0 benchmark
script that satisfies the rules for an application script given in
Section~\ref{sec:logistics}.

Solvers must respond to each command sent by the trace executor with
the answers defined in the SMT-LIB~2.0 format specification, that is,
with a \texttt{success} answer for \akey{set-option},
\akey{set-logic}, \akey{declare-sort}, \akey{declare-fun},
\akey{define-sort}, \akey{define-fun}, \akey{assert}, \akey{push 1},
and \akey{pop 1} commands, and with an answer of \texttt{sat},
\texttt{unsat}, or \texttt{unknown} for \akey{check-sat} commands.
There will be no \akey{get-unsat-core}, \akey{get-proof},
\akey{get-model}, \akey{get-assignments}, or \akey{get-values}
commands in application track benchmarks.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Benchmarks and Problem Divisions}

\header{Benchmark sources.}
%
Benchmarks for each division will be drawn from the SMT-LIB benchmark
repository.  The main track will use a subset of all
\emph{non-incremental} benchmarks; the application track will use a
subset of all \emph{incremental} benchmarks.

\header{New benchmarks.}
%
The deadline for submission of new benchmarks is {\bf May~1, 2015}.
The organizers will be checking and curating these until {\bf June~7,
  2015}.  The organizers reserve the right to exclude new benchmarks
if any prove problematic for some reason.  SMT-COMP attempts to give
preference to benchmarks that are ``real-world,'' in the sense of
coming from or having some intended application outside SMT.

New benchmarks will be made publicly available as soon as possible
after the benchmark submission deadline, as they are checked and
curated.  The set of benchmarks selected for the competition will be
published when the competition begins.

\header{Benchmark demographics.}
%
In SMT-LIB, benchmarks are organized according to \emph{families}.  A
benchmark family contains problems that are similar in some
significant way.  Typically they come from the same source or
application, or are all output by the same tool.  Each top-level
subdirectory within a division represents a distinct family.

Each benchmark in SMT-LIB also has a \emph{category}.  There are four
possible categories:
\begin{itemize}
\item \emph{check.} These benchmarks are hand-crafted to test whether
  solvers support specific features of each division.
\item \emph{industrial.} These benchmarks come from some real
  application and are produced by tools such as bounded model
  checkers, static analyzers, extended static checkers, etc.
\item \emph{random.} These benchmarks are randomly generated.
\item \emph{crafted.} This category is for all other benchmarks.
  Usually, benchmarks in this category are designed to be particularly
  difficult or to test a specific feature of the logic.
\end{itemize}

\header{Benchmark selection.}
%
The benchmark pool is culled as follows:
\begin{enumerate}
\item \emph{Retire inappropriate benchmarks.} The competition
  organizers may remove from the eligibility pool certain benchmarks
  that are inappropriate or uninteresting for competition, or cut the
  size of certain benchmark families to avoid their
  over-representation.
\item \emph{Eliminate benchmarks whose status is unknown.} Any
  benchmark whose expected output is neither {\tt sat} nor {\tt unsat}
  is removed.  There is a significant number of benchmarks with {\tt
    unknown} status in the library; these are certainly interesting
  and are a challenge to solve, but they are not used in the
  competition.\footnote{Incremental benchmarks may contain multiple
    \akey{check-sat} commands, each with its own status.  Incremental
    benchmarks that contain (some) \akey{check-sat} commands with {\tt
      unknown} status may still contain a prefix of commands where the
    status is known for all \akey{check-sat} commands in the prefix.
    This prefix is eligible for the application track.}
\end{enumerate}
%
All remaining benchmarks are used for the competition.  There will be
no further selection of benchmarks, e.g., based on benchmark
difficulty or benchmark category.

\header{Heats.}
%
Since the organizers at this point are unsure how long a set of
benchmarks may take (which will depend also on the number of solvers
submitted), the competition may be run in \emph{heats}.  For each
track and division, the selected benchmarks may be randomly divided
into a number of (possibly unequal-sized) heats.  Heats will be run in
order.  If the organizers determine that there is adequate time, all
heats will be used for the competition.  Otherwise, incomplete heats
will be ignored.

\header{Benchmark scrambling.}
%
Benchmarks will be slightly scrambled before the competition, using a
simple benchmark scrambler.  The benchmark scrambler will be made
publicly available before the competition.

Naturally, solvers must not rely on previously determined identifying
syntactic characteristics of competition benchmarks in testing
satisfiability.  Violation of this rule is considered cheating.

\header{Pseudo-random numbers.}
%
Pseudo-random numbers used, e.g., for the creation of heats or the
scrambling of benchmarks, will be generated using the standard C
library function \texttt{random()}, seeded (using \texttt{srandom()})
with the sum, modulo $2^{30}$, of the integer numbers provided in the
system descriptions (see Section~\ref{sec:entrants}) by all SMT-COMP
entrants other than the organizers'.  Additionally, the integer part
of the opening value of the New York Stock Exchange Composite Index on
the first day the exchange is open on or after the date specified in
the timeline (Section~\ref{sec:important}) will be added to the other
seeding values.  This helps provide transparency, by guaranteeing that
the organizers cannot manipulate the seed in favor of or against any
particular submitted solver.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Scoring}

Scores will be computed for all solvers and divisions.  However,
winners will be declared only for \emph{competitive} divisions.  A
division in a track is competitive if at least two substantially
different solvers (i.e., solvers from two different teams) were
submitted.  Although the organizers may enter other solvers for
comparison purposes, only solvers that are explicitly submitted by
their authors determine whether a division is competitive, and are
eligible to be designated as winners.

Divisions for floating-point arithmetic are experimental in 2015, and
will not be considered competitive.

\subsection{Benchmark scoring}
\label{sec:benchmark-scoring}

A solver's \emph{raw score} for each benchmark is a quadruple $\langle
e, n, w, c\rangle$, with $e \in \{0, 1\}$ the number of erroneous
results (usually~$e = 0$), $0 \leq n \leq N$ the number of correct
results,\footnote{Here, $N$ is the number of \akey{check-sat} commands
  in the benchmark.  Recall that main track benchmarks have just one
  \akey{check-sat} command; application track benchmarks may have
  multiple \akey{check-sat} commands.} $w \in [0,T]$ the (real-valued)
wall-clock time in seconds, and $c \in [0, 4T]$ the (real-valued) CPU
time in seconds, measured across all cores and sub-processes, until
the solver process terminates.

\header{Main track.} More specifically, for the main track, we have
%
\begin{itemize}
\item $e=0$, $n=0$ if the solver times out, or the solver aborts
  without a response, or the result of the \akey{check-sat} command is
  {\tt unknown},
\item $e=0$, $n=1$ if the solver does not time out, and the result of
  the \akey{check-sat} command is correct,
\item $e=1$, $n=0$ if the solver does not time out, and the result of
  the \akey{check-sat} command is incorrect.
\end{itemize}
%
Note that for a (correct or incorrect) result to be taken into
consideration, the solver process must terminate (normally or
abnormally) within the time limit.

\header{Application track.}
%
An application benchmark may contain multiple \akey{check-sat}
commands.  Solvers may partially solve the benchmark before timing
out.  The benchmark is run by the trace executor, measuring the total
time (summed over all individual commands) taken by the solver to
respond to commands.\footnote{Times measured by StarExec may include
  time spent in the trace executor.  We expect that this time will
  likely be insignificant compared to time spent in the solver, and
  nearly constant across solvers.}  Most time will likely be spent in
response to \akey{check-sat} commands, but \akey{assert}, \akey{push}
or \akey{pop} commands might also entail a reasonable amount of
processing.  For the application track, we have
\begin{itemize}
\item $e=1$, $n=0$ if the solver returns an incorrect result for any
  \akey{check-sat} command within the time limit,
\item otherwise, $e=0$, and $n$ is the number of correct results for
  \akey{check-sat} commands returned by the solver before the time
  limit is reached.
\end{itemize}

\subsection{Sequential performance}
\label{sec:sequential}

SMT-COMP has traditionally emphasized sequential performance (i.e.,
CPU time) over parallel performance (i.e., wall-clock time).  StarExec
is measuring both, and we intend to recognize both best sequential and
best parallel solvers in all competitive main track divisions.

The raw score, as defined in Section~\ref{sec:benchmark-scoring},
favors parallel solvers, which may utilize all available processor
cores.  To evaluate sequential performance, we derive a
\emph{sequential score} by imposing a (virtual) CPU time limit equal
to the wall-clock time limit~$T$.  A solver result is taken into
consideration for the sequential score only if the solver process
terminates within this CPU time limit.  More specifically, for a given
raw score $\langle e, n, w, c\rangle$, the corresponding sequential
score is defined as~$\langle e_S, n_S, c_S\rangle$, where
\begin{itemize}
\item $e_S = 0$ and $n_S = 0$ if $c > T$, $e_S = e$ and $n_S = n$
  otherwise,
\item $c_S = \min\ \{c, T\}$.\footnote{\emph{Rationale:} Under this
  score, a solver should not benefit from using multiple processor
  cores.  Conceptually, the sequential score should be (nearly)
  unchanged if the solver was run on a single-core processor, up to a
  time limit of~$T$.}
\end{itemize}

\subsection{Division scoring}
\label{sec:division-scoring}

To compute a solver's score for a division, the solver's individual
benchmark scores for all benchmarks in the division are summed
component-wise.  For main track divisions, we will separately compute
the sum of all raw scores (Section~\ref{sec:benchmark-scoring}) to
assess parallel performance, and the sum of all sequential scores
(Section~\ref{sec:sequential}) to assess sequential performance.  For
application track divisions, division scores will be based on raw
scores.\footnote{Since application track benchmarks may be partially
  solved, defining a useful sequential score for the application track
  would require information not provided by the raw score, e.g.,
  detailed timing information for each result.}

Division scores are compared lexicographically:
\begin{itemize}
\item A sum of raw scores $\langle e, n, w, c\rangle$ is better than
  $\langle e', n', w', c'\rangle$ iff $e < e'$ or ($e = e'$ and $n >
  n'$) or ($e = e'$ and $n = n'$ and $w < w'$) or ($e = e'$ and $n =
  n'$ and $w = w'$ and $c < c'$).  That is, fewer errors takes
  precedence over more correct solutions, which takes precedence over
  less wall-clock time taken, which takes precedence over less CPU
  time taken.
\item A sum of sequential scores $\langle e_S, n_S, c_S\rangle$ is
  better than $\langle e_S', n_S', c_S'\rangle$ iff $e_S < e_S'$ or
  ($e_S = e_S'$ and $n_S > n_S'$) or ($e_S = e_S'$ and $n_S = n_S'$
  and $c_S < c_S'$).  That is, fewer errors takes precedence over more
  correct solutions, which takes precedence over less CPU time taken.
\end{itemize}
%
We will not make any comparisons between raw scores and sequential
scores, as these are intended to measure fundamentally different
performance characteristics.

\subsection{Competition-wide scoring}

We define a competition-wide metric for the main track, separately for
parallel and sequential performance, as follows.  Let $N_i$ be the
total number of benchmarks in division~$i$ that were used in the
competition, and let $\langle e_i, n_i, w_i, c_i\rangle$ be a solver's
raw score for this division (Section~\ref{sec:division-scoring}).  The
solver's competition-wide raw score is $\sum_i (e_i == 0 \,?\,
(n_i/N_i)^2 : -e_i) \log N_i$, where the sum is over all competitive
divisions into which the solver was
entered.\footnote{\emph{Rationale:} This metric purposely emphasizes
  breadth of solver participation---a solver participating in many
  logics need not be the best in any one of them.  The use of the
  square in the metric limits that somewhat---a solver still needs to
  do reasonably well compared to winners to be able to catch up by
  breadth of participation.  The non-linear metric also gives added
  weight to completing close to all benchmarks in a division.  The log
  scaling is a (somewhat arbitrary) means to adjust the scores for the
  wide variety of numbers of benchmarks in different divisions.  It
  seems a reasonable compromise between linearly combining numbers of
  benchmarks, which would overweigh large divisions, and simply
  summing the fraction of benchmarks solved, which would overweigh
  small divisions.  The metric is also quite simple, and the metric
  for a solver is independent of the performance of other solvers.
  Time is omitted from the metric because it is only of third
  importance in the regular competition metric, and is difficult to
  compare across divisions.}  The solver's competition-wide sequential
score is computed from its sequential division scores
(Section~\ref{sec:division-scoring}) according to the same formula.
We will recognize the best three solvers according to these metrics.

\subsection{Other recognitions}

The organizers will also recognize the following contributions:
%
\begin{itemize}
\item \emph{Open source.}  The top solver that provides its source
  code as open source will be recognized in each competitive division.
\item \emph{Best industrial performance.}  There is agreement in the
  SMT community to emphasize problems that come from real
  applications.  The solver that performs best on industrial
  benchmarks, as measured by the division scoring restricted to this
  benchmark category, will be recognized in each competitive division.
\item {\em Best new entrant}. The best performing entrant from a new
  solver implementation team, as measured by the competition-wide
  metric.
\item {\em Breadth of logics}. Solvers that cover the most theories
  and logics.
\item {\em Benchmarks}. Contributors of new benchmarks.
\end{itemize}
%
The organizers reserve the right to recognize other outstanding
contributions that become apparent in the competition results.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Judging}

The organizers reserve the right, with careful deliberation, to remove
a benchmark from the competition results if it is determined that the
benchmark is faulty (e.g., syntactically invalid in a way that affects
some solvers but not others); and to clarify ambiguities in these
rules that are discovered in the course of the competition.  Authors
of solver entrants may appeal to the organizers to request such
decisions.  Organizers that are affiliated with solver entrants will
be recused from these decisions.  The organizers' decisions are final.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Acknowledgments and Disclaimer}

SMT-COMP 2015 is organized under the direction of the SMT Steering
Committee. The organizing team is
%
\begin{itemize}
\setlength{\itemsep}{0pt}
\item \href{https://www.lri.fr/~conchon/}{Sylvain Conchon}~--
  Paris-Sud University, France (co-organizer)
\item \href{https://www.sites.google.com/site/deharbe/}{David
  D{\'e}harbe}~-- Federal University of Rio Grande do Norte, Brazil
  (co-organizer)
\item \href{http://user.it.uu.se/~tjawe125/}{Tjark Weber}~-- Uppsala
  University, Sweden (chair)
\end{itemize}
%
Tjark Weber is responsible for policy and procedure decisions, such as
these rules, with input from the co-organizers. He is not associated
with any group creating or submitting solvers.

Many others have contributed benchmarks, effort, and feedback.  Clark
Barrett processed the newly submitted benchmarks.  The competition
uses the \href{https://www.starexec.org/}{StarExec} service, which is
hosted at the \href{http://www.cs.uiowa.edu/}{University of Iowa}.
Aaron Stump is providing essential StarExec support.

David D{\'e}harbe is associated with the solver group producing the
veriT solver.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bibliographystyle{plain}
\bibliography{biblio}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}
