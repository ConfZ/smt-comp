\documentclass[12pt]{article}
\usepackage{times}
\usepackage{fullpage}
\usepackage{url}
\usepackage{epsf}

\newcommand{\akey}[1]{\textbf{#1}}

\begin{document}

\date{\small This version revised \the\year-\the\month-\the\day}

\title{Satisfiability Modulo Theories Competition (SMT-COMP) 2014: Rules and 
Procedures}


% [morgan] do our own layout of authors; the four-author layout spacing
% was screwed up...
\def\doauthor#1{{%
  \hsize.5\hsize \advance\hsize by-1cm %
  \def\\{\hss\egroup\hbox to\hsize\bgroup\strut\hss}%
  \vbox{\hbox to\hsize\bgroup\strut\hss#1\hss\egroup}}}%

\def\header#1{\medskip\noindent\textbf{#1}}

\author{
David R. Cok \\
GrammaTech, Inc. \\
Ithaca, NY (USA) \\
cok@frontiernet.net \\
\and
David Deharbe \\
Federal University of Rio Grande do Norte \\
Brazil \\
David.Deharbe@loria.fr \\
\and
Tjark Weber \\
Uppsala University, \\
Sweden, \\
tjark.weber@it.uu.se \\ 
}



\maketitle

\def\eg{\textit{e.g.}}
\def\ie{\textit{i.e.}}

\noindent Comments on this document should be emailed to the smtcomp mailing
list or, if necessary, directly to the organizers.

\section{Important dates and information}
\begin{itemize}
\item May 15 - deadline for new benchmarks
\item May 25 - deadline for solver entrants
\item June 1 - deadline for final versions of solvers (but no new entrants after May 25).
\item June 17 - SMT workshop; completion of competition.
\item SMT COMP mailing list: smt-comp@cs.nyu.edu
\item signup site for mailing list: \url{cs.nyu.edu/mailman/listinfo/smt-comp}
\item competition web site: \url{www.smtcomp.org}  or \url{smtcomp.sourceforge.net/2014}
\end{itemize}

\section{Introduction}
\label{sec:intro}

The annual Satisfiability Modulo Theories Competition~(SMT-COMP) is
held to spur advances in SMT solver implementations on benchmark
formulas of practical interest.  Public competitions are a well-known
means of stimulating advancement in software tools.  For example, in
automated reasoning, the CASC and SAT competitions for first-order and
propositional reasoning tools, respectively, have spurred significant
innovation in their fields~\cite{PSS02,leberre+03}.  More information
on the history and motivation for SMT-COMP can be found at the
SMT-COMP web site, \url{www.smtcomp.org}, and in reports on previous
competitions~(\cite{SMTCOMP-2008,BDOS08,SMTCOMP-FMSD,SMTCOMP-JAR,TBD-2012,TBD-2013}).
SMT-COMP 2014 is affiliated with the SMT workshop (\url {http://smt2014.it.uu.se/}) at the co-located meetings of CAV, IJCAR, and SAT at FLoC 2014 (\url{http://vsl2014.at/}); the competition is part of the
Olympic Games (\url{http://vsl2014.at/olympics/}).

Accordingly, researchers are highly encouraged to submit both new benchmarks
and new or improved solvers to raise the level of competition and advance
the state-of-the-art in automated SMT problem solving.

SMTCOMP 2014 will have two tracks: the conventional main track and a repeat of 2012's application track.
Within a track there are one or more divisions, where each division
uses benchmarks from a specific SMT-LIB logic (or group of logics).

The rest of this document, revised from the previous year's
version\footnote{Earlier versions of this document include contributions from
Clark Barrett, Albert Oliveras, Aaron Stump, Morgan Deters, David Cok, Alberto Griggio, and Roberto Bruttomesso.},
describes the rules and competition procedures for SMT-COMP~2014.
The principal changes from last competition are the following:
\begin{itemize}
\item The competition will be run with StarExec. Accordingly we are simplifying the competition somewhat, since some of the infrastructure needs to be recreated.
\item We will hold just a main and application track. [ The demonstration parallel solvers, unsat core, and proof generation tracks will be deferred until benchmarks and result analysis tools are reorganized.]
\item In the main track, we will measure comparative performance within all divisions to which sufficient solvers are submitted.

\item We are retaining and accenting last year's ``application track" and
  encourage submission of benchmarks for this track and competition against these
  benchmarks.

\item In addition to the traditional SMTCOMP winners, we will recognize winners in other categories, as described below. 
Also, importantly, the Olympic Games has an award ceremony (with actual medals). A selection of the SMTCOMP winners (as described below) will be awarded Olympic Games medals.

\end{itemize}

All results of the competition will be made public. However, 
winners will be declared only for divisions in which at least two solvers are submitted.
Although the organizers may add other solvers for comparison purposes, only solvers that
are explicitly submitted by their authors' are eligible to be designated as winners.
If necessary, and based on final submissions, the organizers may elect to
combine competition divisions to make them competitive (such decisions will
be made with input from the community).

It is important for competitors to track discussions on the SMT-COMP mailing
list, as clarifications and any updates to these rules will be posted there.

\section{Entrants}
\label{sec:entrants}

\header{Solver format.} %
An entrant to SMT-COMP is an SMT solver
submitted by its authors using the Star-Exec (\url{http://www.starexec.org}) service.  
The execution service enables members of the
SMT research community to run solvers on jobs consisting of benchmarks
from the SMT-LIB benchmark library.  Jobs are run on a shared computer
cluster.  The execution service is provided free of
charge, but it does require registration to create a login account.  Registered users
may then upload their own solvers to run, or may run public solvers
already uploaded to the service.  


For participation in SMT-COMP, a solver must be
uploaded to Star-Exec, made publicly available, and the organizers informed of its presence {|em and thte logic divisions in which it is participating}.

A submission must also include a 32-bit unsigned
integer.  These numbers, collected from all submissions, are used
to seed the pseudo-random benchmark selection algorithm, as well
as the benchmark scrambler.

\header{System description.} %
As part of a submission,
SMT-COMP entrants are encouraged to provide a short (1--2 pages) description of
the system.  This should include a list of all authors of the system
and their present institutional affiliations.  The programming
language(s) and basic SMT solving approach employed should be
described (\eg, lazy integration of a Nelson-Oppen combination with
SAT, translation to SAT, etc.).  System descriptions are encouraged to
include a URL for a web site for the submitted tool, but this is
optional.  

\header{Other systems.} %
As in previous years, due to limitations on
computational resources, the organizers reserve the right not to
accept multiple versions of the same solver (defined as sharing 50\%
or more of its source code).  The organizers reserve the right to
include other systems of interest (such as entrants in previous competitions), to the
competition.

\header{Wrapper tools.} %
A \emph{wrapper tool} is defined as any tool
which calls one or more SMT solvers not written by the author of the wrapper
tool.  The other solvers are is called the \emph{wrapped} tools.  
A wrapper tool must explicitly acknowledge any tools that it wraps.
Its system description should make clear the technical innovations  by which the
wrapper tool expects to improve on the wrapped tools.

\header{Attendance.} %
As with previous SMT-COMPs, submitters of an SMT-COMP entrant need not 
be physically present at the competition to
participate or win.

\subsection*{Deadlines} %

\paragraph{Main competition track.} %
SMT-COMP entries must be submitted via StarExec and email to the organizers by 7pm, Eastern U.S.
time, May~25, 2014.  At that time new entrants will be
closed to the public to prepare for the competition, with the
exception that resubmissions of existing entries will be accepted
until 7pm, Eastern U.S. time, June~1, 2014.  We strongly encourage
participants to use this grace period \emph{only} for the
purpose of fixing any bugs that may be discovered and not for adding
new features as there will be no opportunity to do extensive testing
using StarExec after the original deadline on May~25.

The versions that are present on the execution service at the conclusion of the
grace period will be the ones used for the competition, and versions
submitted after this time will not be used.  The organizers reserve
the right to start the competition itself at any time after the open
of the New York Stock Exchange on the morning of June~2.  See
Section~\ref{sec:timeline} below for a full timeline.

\paragraph{Parallel track}
StarExec reports both cpu time and wall clock time. The main competition will be measured by cpu time taken across all processes and threads that are spawned by the starting process. For single-process executions the wall-clock time is typically slightly larger than the cpu time. Multi-threaded solvers will see reduced wall-clock times. While we will not run an explicit parallel track in 2014. We will recognize the solvers with the best wall-clock time in each division.

\paragraph{Application track.} %
The same deadlines and procedure for submitting to the main track will be used
also for the application track. Submissions to both the application track and
the main competition are independent: participants must submit explicitly to
both events to participate in both, and they may submit different (or
differently configured) solvers to each.  Benchmarks will be scrambled also
for this division, using the same scrambler and seed as the main track.
Entrants should still include a system description, as for the regular
competition.

The infrastructure for the application track is in the process of being recreated for StarExec.
If it is not ready in time, the organizers may choose to drop this track or only run it as a demonstration track.

\section{Execution of Solvers}
\label{sec:exec}

Solvers will be publicly evaluated in the following tracks. In exhibition divisions of these tracks, solvers are evaluated and results posted, but no awards are announced. The other divisions are competition divisions; if there are sufficient entrants, a winner will be announced.
\begin{itemize}
\item a main track: sequential execution evaluated separately on benchmarks from each of the logics for which there are benchmarks in StarExec.
\item application track: evaluation on command scripts (all divisions with sufficient benchmarks and entrants are competitive)
\end{itemize}

\subsection{Logistics}

\header{Dates of competition.}
%
We anticipate that the bulk of the main track of the competition will take place during
the weeks leading up to SMT 2014, from about June~2 to the July~17.  Results will be
announced at SMT 2014 and in an awards ceremony scheduled by the Olympic Games coordinator.
  Intermediate results
will be regularly posted to the SMT-COMP website as the competition
runs.


The organizers will prioritize the running of the competition tracks, and may shift the
time period or order of the competition or exhibition tracks in order to complete SMT-COMP
prior to the SMT workshop and awards ceremony.

\header{Input and Output.}
%
Participating solvers must read a single benchmark script (defined
below, not part of the 2.0 standard), presented on its standard input
channel. The script is in the concrete syntax of the SMT-LIB format,
version 2.0.   A
\textbf{benchmark script} is just a text file containing a sequence of SMTLIBv2 commands in which:

\begin{enumerate}
\item The (single) $\akey{set-logic}$ command setting the benchmark's
logic is the first command after any $\akey{set-option}$ commands described below.
% rb 25/3: added
% \item A (single) \akey{set-option :print-success false} command, sent just
%       after $\akey{set-logic}$. This is to
%       avoid the output ``\texttt{success}'' to interfere with the
%       answer of the solver to the benchmark. 
\item The $\akey{exit}$ command is the last command.
\item For tracks other than the application track, there is exactly one $\akey{check-sat}$ command,
following possibly several $\akey{assert}$ commands.
\item For the application track, there are one or more \akey{check-sat} commands, 
  each preceded by one or more \akey{assert} commands 
  and zero or more \akey{push 1} commands, 
  and followed by zero or more \akey{pop 1} commands.
\item Scripts for the application track will have an initial $\akey{set-option :print-success true}$ command.
\item There is at most one $\akey{set-info}$ command for \texttt{status}.
%%%%%%%%%%%%%%%%%%% TODO: Need to define the set-info command.
\item The formulas in the script belong to the benchmark's logic, with
any free symbols declared in the script.
\item Extra symbols are declared exactly once before any
  use, using $\akey{declare-sort}$
  %, $\akey{define-sort}$,
  %$\akey{declare-fun}$, or $\akey{define-fun}$.  
  or $\akey{declare-fun}$.
  They must be part of the allowed signature expansion for the logic.
  Moreover, all sorts declared with a $\akey{declare-sort}$ command must have zero arity.


\item No other commands besides the ones just mentioned may be used.

\end{enumerate}

  
\noindent The SMT-LIB format specification is publicly
available from the ``Documents'' section of the SMT-LIB
website~\cite{SMT-LIB}.  Solvers will be given formulas just from the
Problem Divisions indicated during their submission to SMT-Exec.
Example benchmark scripts for several Problem Divisions are reported in the Appendix.
Note that they are provided for illustrative purposes only: 
please refer to the SMT-LIB format specification 
and the above definition of benchmark script for the official specification 
of the input format for SMT-COMP.

\subsection{Main track}
\label{sec:exec:main}

The main track competition will consist of selected benchmarks in each of the
logic divisions.
Each benchmark script will be presented to the standard input of the solver.
Each SMT-COMP entrant is then expected to attempt to report on its
standard output channel whether the formula is satisfiable
(``\texttt{sat}'', in lowercase, without the quotation marks) or unsatisfiable
(``\texttt{unsat}'').  An entrant may also report ``\texttt{unknown}''
to indicate that it cannot determine satisfiability of the formula.

The main track competition uses a StarExec postprocessor (SMT2 results conserv) to accumulate the results.

Solvers that register in non-competition logic divisions (cf. section \ref{sec:theories})
will be run in an exhibition mode: results will be generated and publicly
posted, but no awards will be announced.

\header{Timeouts.} %
Each SMT-COMP solver will be executed on an
unloaded competition machine for each given formula, up to a fixed
time limit.  The time limit is yet to be determined, but it is
anticipated to be 1500 seconds (25 minutes) of wall clock time and 6000 seconds of CPU time.\footnote{The time limit may be adjusted once we understand the resources and capabilities of the StarExec service and the number of competition solvers.} 
% TBD- TODO - how to constrain solvers to single processes?
Solvers that take more than this
time limit will be killed.  Solvers are allowed to spawn other
processes.  These will be killed at approximately the same time as the
first started process.  A
timeout scores the same as if the output is ``\texttt{unknown}''.


\header{Aborts and unparsable output.} %
Solvers which exit before the time
limit without reporting a result (\ie\ due to exhausting memory, crashing,
or producing output other than \texttt{sat}, \texttt{unsat}, or
\texttt{unknown})
will be considered to have aborted. 
An abort scores the same as if the output is ``\texttt{unknown}''. 
Also, as a further measure to prevent misjudgments of solvers,
any  ``\texttt{success}'' outputs will be 
ignored.\footnote{
Note that SMT-LIBv2 requires that a solver produce a ``\texttt{success}'' answer
after each \akey{set-logic}, \akey{declare-sort}, \akey{declare-fun} and
\akey{assert} command (among others), unless the option
\akey{:print-success} is set to false; ignoring the
\texttt{success} outputs therefore allows for submitting fully-compliant
solvers without the need of a wrapper script, while still allowing entrants
of previous competitions to run without changes.}

\header{Persistent state.} %
Solvers are allowed to create and write to
files and directories during the course of an execution, but they are
not allowed to read such files back during later executions.  Any
files written should be put in the directory in which the tool is
started, or in a subdirectory.

\subsection{Application track}
\label{sec:exec:application}

The application track evaluates SMT solvers when interacting
with an external verification framework, \eg, a model
checker. This interaction, ideally, happens by means of an online
communication between the model checker and the solver: the model
checker repeatedly sends queries to the SMT solver, which in turn
answers either \texttt{sat} or \texttt{unsat}.  In this interaction an SMT-solver is
required to accept queries incrementally via its standard input channel.

In order to facilitate the evaluation of the solvers in this track, we
will set up a ``simulation'' of the aforementioned interaction, as was done in 2012. In
particular each benchmark in the application track represents a realistic
communication trace, containing multiple \akey{check-sat} commands (possibly
with corresponding \akey{push 1}/\akey{pop 1} commands), which
is parsed by a {\em trace executor}. The trace executor serves the following purposes:
\begin{itemize}
\item it simulates the online interaction by sending single queries to the SMT solver
      (through stdin);
\item it prevents ``look-ahead'' behaviours of SMT solvers;
\item it records time and answers for each call, possibly aborting the execution
      in case of a wrong answer;
\item it guarantees a fair execution for all solvers by abstracting from any possible
      crash, misbehaviour, etc. that may happen on the model checker side.
\end{itemize}

\header{Input and Output.}
Participating solvers will be connected to a trace executor 
which will incrementally send commands to the standard input channel of the solver
and read responses from the standard output channel of the solver.
The commands will be taken from an \textbf{incremental benchmark script},
which is an SMT-LIB 2.0 script which satisfies the rules for an application script given above.
Note also that the trace executor will send a single 
\akey{set-option :print-success true} command to the solver before 
sending commands from the incremental benchmark script.

\medskip
Solvers must respond immediately to the commands sent by the trace executor, 
with the answers defined in the SMT-LIB 2.0 format specification, that is,
with a \texttt{success} answer for 
\akey{set-option}, 
\akey{set-logic}, 
\akey{declare-sort}, 
\akey{declare-fun}, 
\akey{assert}, 
\akey{push 1}, 
and \akey{pop 1} 
commands,
 with a \texttt{sat}, \texttt{unsat}, or \texttt{unknown} 
for \akey{check-sat} commands, and with the defined responses for \akey{get-unsat-core} and \akey{get-proof} commands.

\header{Timeouts.}
A time limit is set for the whole execution of each application
benchmark (consisting of multiple queries).  We anticipate the
timeout to be around 25~minutes (as it has been in the past).\footnote{The timeout may be adjusted once we have experience with the abilities and resources of the StarExec service.}



\section{Benchmarks and Problem Divisions}
\label{sec:theories}

In 2012 the competition ran only some logics as competitive divisions.
In 2014 we will revert to previous practice and run all logics as competitive divisions, if there are multiple solvers submitted.

The organizers reserve the right to include in the benchmark population for a given logic benchmarks
from subsumed logics. For example, QF\_UFLIA may include some samples from QF\_LIA, and QF\_UFLRA may include some from QF\_LRA.
We will not use benchmarks from QF\_UF in other divisions.

\subsection{Main and parallel tracks}

\header{Benchmark sources.} %
Benchmark formulas for these divisions
will be drawn from the SMT-LIB library. The deadline for new benchmarks is
May 15. The organizers will be checking and curating these until June 1;
the organizers reserve the option to exclude new benchmarks if any prove problematic for some reason.
SMT-COMP
attempts to give preference to benchmarks that are ``real-world,'' in
the sense of coming from or having some intended application outside
SMT.

\header{Benchmark availability.} % 
A first release of the competition
benchmarks will be made available by May 15, 2014.
A second and almost final release will be available on June 1st. 
No additional
benchmarks will be added after this date, but benchmarks may be
modified or removed to fix possible bugs or other issues, or to adjust their difficulty score (see below). 
The final release that will be used for the competition will be posted on June
1. The set of selected benchmarks will be published when the
competition begins.

\header{Benchmark demographics.} %
In SMT-LIB, benchmarks are organized according to \emph{families}.  A benchmark
family contains problems that are similar in some significant way.  Typically
they come from the same source or application, or are all output by the same
tool.  \emph{Each top-level subdirectory within a division represents a distinct
family.}  
%

Each benchmark in SMT-LIB also has a \emph{category}.  There are four possible
categories:
%
\begin{itemize}
\item \emph{check.} These benchmarks are hand-crafted to test whether
  solvers support specific features of each division.  In particular,
  there are checks for integer completeness (\ie\ benchmarks that are
  satisfiable under the reals but not under the integers) and big
  number support (\ie\ benchmarks that are likely to fail if integers
  cannot be represented beyond some maximum value, such as
  $2^{31}-1$).

%  \noindent%\textbf{New for 2010:} 
%  Using the same random seed as
%  for benchmark selection and scrambling, 5 ``check'' benchmarks will
%  be randomly generated using Robert Brummayer's SMT fuzzing
%  tool~\cite{brummayer+09}.  The (exact version of this) tool will be
%  publicly available from the SMT-COMP~2012 web site before the
%  competition.  Since these benchmarks are generated after the random
%  seed is fixed, they cannot be known in advance to any competitors,
%  including the organizers.  The rationale for including these
%  benchmarks is to try to take a step towards stronger certification
%  that solvers are correct.  In future years, we envision SMT-COMP
%  requiring that solvers pass some kind of pre-qualifying round based
%  on SMT fuzzing.  The inclusion of these fuzzing benchmarks that are
%  not known in advance is intended to encourage (without requiring)
%  solver implementors to test their solvers using fuzzing or similar
%  bug-discovery techniques.
  
\item \emph{industrial.} These benchmarks come from some real application
      and are produced by tools such as bounded model checkers, static analyzers, extended
      static checkers, etc.
\item \emph{random.} These benchmarks are randomly generated.
\item \emph{crafted.} This category is for all other benchmarks.  Usually,
  benchmarks in this category are designed to be particularly difficult or to
  test a specific feature of the logic.
\end{itemize}

\header{Benchmark selection.} %

The benchmarks are assigned a difficulty. For 2014, the difficulty measure is the
cpu time taken by the best performing solver in the 2013 SMTEVAL. These times will be posted 
as soon after May 15 as possible.

The selection of benchmarks for the competition will be biased towards more difficult benchmarks
as follows. First, the benchmark pool is culled as follows:
\begin{enumerate}

\item \textbf{Retire very easy benchmarks.} %
  Eliminate benchmarks for which all of the current solvers in SMTEVAL-2103
  solved the benchmark in under 5 seconds,
  \emph{unless} doing so reduces the pool of
    benchmarks for the division to less than~300.

\item \textbf{Retire inappropriate benchmarks.} %
  The competition organizers will remove from the eligibility pool
  certain SMT-LIB benchmarks that are inappropriate or uninteresting
  for competition, or cut the size of certain benchmark families to
  avoid their over-representation.

\end{enumerate}

Then, for each division, the benchmarks are divided into quintiles according to the difficulty measure.
Then a random selection of N competition benchmarks is made as follows:
\begin{itemize}
\item N*40\% are taken from the top quintile, if sufficient benchmarks are available
\item additional benchmarks are taken from the second quintile, to produce N*55\% benchmarks, if sufficient benchmarks are available
\item additional benchmarks are taken from the third quintile, to produce N*70\% benchmarks, if sufficient benchmarks are available
\item additional benchmarks are taken from the fourth quintile, to produce N*85\% benchmarks, if sufficient benchmarks are available
\item any remaining needed benchmarks are taken from the bottom quintile.
\end{itemize}
Within each quintile selection, if sufficient benchmarks are available, 85\% will be chosen from the industrial benchmarks,

Since the organizers at this point are unsure how long a set of benchmarks may take (which will depend also on the number of solvers submitted), the competition will be run in {\em heats}. Some number $N$ of benchmarks will be selected and the competition run. If there is time, an additional $N_1$ benchmarks will be run, etc., until the end of the competition is reached.

%Before the selection process, each benchmark will be assigned a
%\emph{difficulty:} a number between 0.0 and 5.0 inclusive, calculated as in previous years.  The
%difficulty for a particular benchmark will be assigned by running SMT
%solvers from previous competitions that finished in good
%standing and using the formula:
%
%\[\mathrm{difficulty} = {5\cdot\ln\!\left(1+A^2\right) \over \ln\!\left(1+30^2\right)}\]
%
%\noindent
%where $A$ is the average time for the solvers to correctly solve the
%instance (in minutes).  This computation of difficulties
%replaces a simpler formulation in earlier SMT-COMPs that didn't take
%into account the time solvers take.  This calculation of difficulty
%recognizes that problems requiring more time by many solvers are
%more difficult problems.  The logarithm is used to
%mark a larger change in difficulty (given a corresponding increase in
%solver average time) at smaller time scales than at higher ones (if $A=1$,
%difficulty is 0.5; at $A=1.7$, difficulty is 1.0; but a difficulty of 2.0
%requires that $A=4$); the square is used to flatten out this curve sightly
%at the low end.  
%%%Figure~\ref{difficultyplot} shows this curve.
%%%
%%%\begin{figure}
%%%  \centerline{\epsffile{difficultyplot.eps}}
%%%  \caption{The shape of the difficulty assignment curve.}
%%%  \label{difficultyplot}
%%%\end{figure}
%
%When 5 or more solvers are used for the
%calculation, the maximum and the minimum times are dropped from the calculation of the average.  Solvers giving an
%incorrect answer are not counted in the average; solvers crashing,
%timing out, or giving an unknown result are considered as taking 30
%minutes (which, with the above formula, pulls the difficulty
%toward~5.0).  If there are available solvers, but no average is
%defined under these rules, the difficulty shall be 5.0.  For new
%divisions, where there are no available solvers to compute the
%difficulty, the difficulty will be computed using whatever means are
%available to the organizers for that purpose.
%
%The following scheme will be used to choose competition benchmarks
%within each division.  Unknown-status benchmarks from SMT-LIB are
%considered ineligible for competition and are not used.
%
%The selection
%is implemented by our benchmark selection tool, source for which will
%be available at \url{www.smtcomp.org}.
%%%% TODO: Make sure this tool is available.
%
%\begin{enumerate}
%
%\item \textbf{Check benchmarks included.} %
%  All benchmarks in category \emph{check} are included.
%
%\item \textbf{Retire very easy benchmarks.} %
%  The most difficult~300 non-check non-unknown benchmarks in each
%  division are always included, together with all benchmarks on which
%  at least one 2011 solver required more than 5~seconds.  \emph{This
%    is intended to have the effect of retiring ``very easy''
%    benchmarks that were solved by every 2011 solver in less
%    than 5~seconds, \emph{unless} doing so reduces the pool of
%    benchmarks for the division to less than~300.}
%
%\item \textbf{Retire inappropriate benchmarks.} %
%  The competition organizers will remove from the eligibility pool
%  certain SMT-LIB benchmarks that are inappropriate or uninteresting
%  for competition, or cut the size of certain benchmark families to
%  avoid their over-representation.
%
%\item\label{step:pool} \textbf{Division selection pools created.} %
%  For non-\emph{check} benchmarks, selection pools are created.
%  For benchmark families with $\leq200$ eligible,
%  non-\emph{check} benchmarks, all are added to this pool;
%  otherwise, 200 such benchmarks are added to the pool with the
%  following distribution:
%%
%  \begin{itemize}
%  \item 20 with solution \textbf{sat} and difficulty on $\left[0,1\right]$
%  \item 20 with solution \textbf{sat} and difficulty on $\left(1,2\right]$
%  \item 20 with solution \textbf{sat} and difficulty on $\left(2,3\right]$
%  \item 20 with solution \textbf{sat} and difficulty on $\left(3,4\right]$
%  \item 20 with solution \textbf{sat} and difficulty on $\left(4,5\right]$
%  \item 20 with solution \textbf{unsat} and difficulty on $\left[0,1\right]$
%  \item 20 with solution \textbf{unsat} and difficulty on $\left(1,2\right]$
%  \item 20 with solution \textbf{unsat} and difficulty on $\left(2,3\right]$
%  \item 20 with solution \textbf{unsat} and difficulty on $\left(3,4\right]$
%  \item 20 with solution \textbf{unsat} and difficulty on $\left(4,5\right]$
%  \end{itemize}
%%
%  If 20 are not available in one of these subdivisions, all that are
%  available are added, and remaining slots are reallocated to the
%  others.  This process is iterated so that it is guaranteed that 200
%  benchmarks from the benchmark family are in the selection pool, in
%  equal numbers from each subdivision, so far as possible.
%  (In cases where, \eg, there are only two available slots and
%  they can be allocated to one of three subdivisions, they are allocated
%  randomly but are guaranteed to be allocated to \emph{distinct}
%  subdivisions.)
%
%\item\textbf{Category slot allocation.} %
%  Next, 200 slots are allocated for the division as follows:\footnote{The
%  number ``200'' is a guideline, and is expected to be used.  However, the
%  SMT-COMP organizers reserve the right to reduce this to an appropriate
%  value to ensure a timely end to the competition, and may do so on a
%  per-division basis.  The category allotments, etc., will remain the same
%  proportion of the total.}
%%
%  \begin{itemize}
%  \item 170 from category \emph{industrial}
%  \item 20 from category \emph{crafted}
%  \item 10 from category \emph{random}
%  \end{itemize}
%%
%  If there are fewer than 20 (respectively, 10) \emph{crafted} or
%  \emph{random} benchmarks in the division pool, more
%  \emph{industrial} slots are allocated to make 200 total for the
%  division.  If there are too few \emph{industrial} benchmarks in the
%  division pool, more \emph{crafted} slots are allocated to make 200
%  total for the division.  (In no division are there not enough of
%  both industrial and crafted benchmarks.)
%
%\item\textbf{Category subdivision slot allocation.} %
%  For each category, given that it has $n$ slots allocated to it,
%  the slot allocation is further subdivided as follows:
%%
%  \begin{itemize}
%  \item $\left\lfloor n\over10\right\rfloor$ slots for solution \textbf{sat} with difficulty on $\left[0,1\right]$
%  \item $\left\lfloor n\over10\right\rfloor$ slots for solution \textbf{sat} with difficulty on $\left(1,2\right]$
%  \item $\left\lfloor n\over10\right\rfloor$ slots for solution \textbf{sat} with difficulty on $\left(2,3\right]$
%  \item $\left\lfloor n\over10\right\rfloor$ slots for solution \textbf{sat} with difficulty on $\left(3,4\right]$
%  \item $\left\lfloor n\over10\right\rfloor$ slots for solution \textbf{sat} with difficulty on $\left(4,5\right]$
%  \item $\left\lfloor n\over10\right\rfloor$ slots for solution \textbf{unsat} with difficulty on $\left[0,1\right]$
%  \item $\left\lfloor n\over10\right\rfloor$ slots for solution \textbf{unsat} with difficulty on $\left(1,2\right]$
%  \item $\left\lfloor n\over10\right\rfloor$ slots for solution \textbf{unsat} with difficulty on $\left(2,3\right]$
%  \item $\left\lfloor n\over10\right\rfloor$ slots for solution \textbf{unsat} with difficulty on $\left(3,4\right]$
%  \item $\left\lfloor n\over10\right\rfloor$ slots for solution \textbf{unsat} with difficulty on $\left(4,5\right]$
%  \end{itemize}
%%
%  Remaining slots are allocated randomly to distinct subdivisions.
%  If there aren't enough benchmarks in the pool meeting one or more of
%  the above subdivision requirements for the category, the subdivision
%  allocation is reduced to the number available in the pool that meet
%  the requirements.  To make up the full category allotment, remaining
%  slots are allocated equally to subdivisions with enough benchmarks
%  in the pool meeting their requirements.  This process ensures that
%  all slots can be filled with benchmarks from the pool.
%
%\item\textbf{Benchmark selection.} %
%  Benchmarks from the pool are assigned randomly to slots.
%\end{enumerate}
%%
%In the end, up to 200 non-\emph{check} benchmarks per division are
%included in the competition, together with all the \emph{check}
%benchmarks.  Some divisions may have fewer than 200 non-\emph{check}
%benchmarks, in which case all of them are included using this selection
%scheme.
%
%Once the StarExec service is evaluated, and if the organizers decide that
%the service has sufficient resources, the target number of benchmarks for 
%a division may be increased above 200, with all subpopulations being enlarged
%proportionately, insofar as there are sufficient benchmarks. The target number
%of benchmarks will not be raised if that requires reducing the timeout limits
%below the values used in 2011.

The main purpose of the algorithm above is to have a balanced and complete set
of benchmarks.  The  built-in bias is towards industrial rather than crafted
or random benchmarks and towards more difficult benchmarks.
  This reflects a desire by the organizers and agreed upon
by the SMT community to emphasize problems that come from real applications.

Pseudo-random numbers will be generated using the standard C library
function \texttt{random()}, seeded (using \texttt{srandom()}) with the
sum, modulo $2^{30}$, of the numbers provided in the system
descriptions (see Section~\ref{sec:entrants} above) by all SMT-COMP
entrants other than the organizers.  Additionally, the integer part of
the opening value of the New York Stock Exchange Composite Index on
a publicized day
will be added to the other seeding values.  This helps provide transparency,
by guaranteeing that the organizers cannot manipulate the seed in
favor of or against any particular submitted solver.  Benchmarks will also be slightly
scrambled before the competition, using a simple benchmark scrambler
seeded with the same seed as the benchmark selector.  Both the
scrambler and benchmark selector will be publicly available before the
competition.  Naturally, solvers must not rely on previously
determined identifying syntactic characteristics of competition
benchmarks in testing satisfiability (violation of this is considered
cheating).

\subsection{Application track}

\header{Benchmark sources.} %
Benchmarks for the application track will be collected by the SMT-COMP organizers.
Any benchmark available to the organizers by June 1st will be considered eligible.

\header{Benchmark availability.} 
A first release of the application track
benchmarks will be made available May 15, 2014.
More benchmarks can be collected, until June 1.
No additional
benchmarks will be added after this date, but benchmarks can be
modified or removed to fix possible bugs or other issues. 
The final release that will be used for the competition will be posted on 
June 1.

\header{Benchmark demographics and selection.} 
A random selection of all the available benchmarks will be used for the competition. 
As was the case in 2012, no difficulty will be assigned to the benchmarks for the application track.
Benchmarks will be slightly scrambled before the competition, using the same scrambler and 
random seed as the main track. 
If the organizers determine that there is adequate time to complete the competition, all of the benchmarks will be used. Otherwise a unbiased random subset will be used to ensure timely completion of the competition.

\section{Judging and Scoring}
\label{sec:judging}

\subsection{Main and application tracks}
\header{Scoring}
The score for each benchmark is a triple $\langle e,n,m\rangle$, with
$e$ a non-negative number of erroneous results,
$n\in[0,N]$ an integral number of points scored for the benchmark,
where $N$ is the number of \akey{check-sat} commands
in the benchmark, and $m\in[0,T]$ is the (real-valued) (TBD - which kind of time) time in seconds, where $T$ is
the timeout.  Recall that main track benchmarks will have just one \akey{check-sat} command;
application track benchmarks may have multiple \akey{check-sat} commands.
The score for the benchmark is initialized with
$\langle0,0,0\rangle$ and then computed as follows.
\begin{itemize}
\item A correctly-reported \texttt{sat} or \texttt{unsat} answer after
  $s$~seconds (counting from the beginning of this particular
  \akey{check-sat}) contributes $\langle0,1,s\rangle$ to the running
  score.
\item An answer of \texttt{unknown}, an unexpected answer, a crash, or a memory-out during
  execution of the query, or a benchmark timeout, aborts the execution
  of the benchmark and assigns the current value of the running score
  to the benchmark.  (Recall that there is one timeout for the entire
  benchmark; there are no individual timeouts for queries.)
\item An incorrect answer to a \akey{check-sat} command has the effect of terminating the
  evaluation of that individual benchmark, and the returned score for the benchmark
  will be $\langle1,0,0\rangle$.
\end{itemize}


For example, if a benchmark has 5~\akey{check-sat} commands, and a
timeout of 100~seconds, and a solver solves the first four in
10~seconds each, then times out on the fifth, then the solver's score
is $\langle0,4,40\rangle$.  If another solver solves each of the first
four in 10~seconds each, and the fifth in another 40~seconds, its
score is $\langle0,5,80\rangle$.  If a third solver solves the first
four queries in 2~seconds each, but incorrectly answers the fifth, its
score is $\langle1,0,0\rangle$.

As queries are only presented in order, this scoring system may mean
that relatively ``easier'' queries are hidden behind more difficult
ones located at the middle of the query sequence.

Benchmarks' scores are summed componentwise to form a solver's total
score for the competition.
Total scores are compared lexicographically---a score $\langle e,n,m\rangle$ is better than 
$\langle e',n',m'\rangle$ iff $e < e'$ or ($e = e'$ and $n > n'$) or ($e = e'$ and $n = n'$ and $m < m'$).
That is, fewer errors takes precedence over more correct solutions, which takes precedence over less time taken.


\header{Number of competitors}
Winners in each competition Problem Division for which there are at least three
entrants from distinct research groups competing will be taken to be
those with the highest score and no erroneous results. In addition to recognizing the overall
winner in each division, the top solver that provides its source code will also be
recognized in each division.  
For Problem Divisions with fewer than three
entrants, the results will be reported but no winner officially
declared.

\subsection{Other recognitions}
The organizers will also recognize the following contribution:
\begin{itemize}
\item {\em Best new entrant}. The best performing entrant from a new solver implementation team. as measured by the average of 
(solver time)/(winning solver time) over the competition benchmarks.
\item Others TBD
\end{itemize}

\subsection{Olympic Games medals}

TBD

\section{Timeline}
\label{sec:timeline}

\nobreak
\vbox{% no page break here, please!
\begin{description}
\item[May 15] Deadline for benchmark contributions.
  First version of the benchmark scrambler, benchmark selector and trace executor made available.
\item[May 25] First versions of solver competitors are due to the organizers.
\item[June 1] Final version of the benchmark scrambler, benchmark selector and trace executor made available.
\item[June 1] Benchmark libraries are frozen; final versions of solver competitors due.
\item[June 1 (7pm EDT)] Final versions of solvers due via StarExec (for
  all tracks), including divisions being entered, system
  descriptions and magic numbers for benchmark scrambling.
\item[June 2] Opening value of NYSE Composite Index used to complete random seed.
\item[July 17] SMT 2014; end of competition.
\end{description}}

\section{Mailing List}
\label{sec:ps}

Interested parties should subscribe to the SMT-COMP mailing list:
.  Important
late-breaking news and any necessary clarifications and edits to these
rules will be announced there, and it is the primary way that such
announcements will be communicated.

\section{Disclaimer}
\begin{itemize}
\item David Cok is the chief organizer of SMT-COMP 2014. He is 
responsible for all policy and procedure decisions, such as these
rules. He is not associated 
with any group creating or submitting solvers. He has used solvers
in industrial settings and is keenly interested to know which are the best.

\item David Deharbe and Tjark Weber are co-organizers. 
They will  
be validating the competition setup, checking benchmarks, and
operating the competition. Deharbe is associated with the solver group producing the veriT solver.
\end{itemize}
\bibliographystyle{plain}
\bibliography{biblio}

\appendix
\section{Sample benchmark scripts for the main track}

\header{QF\_UF}

{\footnotesize
\begin{verbatim}
(set-logic QF_UF)
(set-info :status sat)
(declare-sort U 0)
(declare-fun f (U) U)
(declare-fun g (U) U)
(declare-fun A () Bool)
(declare-fun x () U)
(declare-fun y () U)
(assert
(let ((fx (f x))
      (cls1 (or A (= x y))))
  (and cls1 (distinct fx (g y)))))
(check-sat)
(exit)
\end{verbatim}}


\header{QF\_LRA}

{\footnotesize
\begin{verbatim}
(set-logic QF_LRA)
(declare-fun x () Real)
(declare-fun y () Real)
(declare-fun A () Bool)
(assert
  (let ((i1 (ite A (<= (+ (* 2.0 x) (* (/ 1 3) y)) (- 4))
                   (= (* y 1.5) (- 2 x)))))
    (and
      i1
      (or (> x y) (= A (< (* 3 x) (+ (- 1) (* (/ 1 5) (+ x y)))))))))
(check-sat)
(exit)
\end{verbatim}}


\header{QF\_LIA}

{\footnotesize
\begin{verbatim}
(set-logic QF_LIA)
(declare-fun x () Int)
(declare-fun y () Int)
(declare-fun A () Bool)
(assert
  (let ((i1 (ite A (<= (+ (* 2 x) (* (- 1) y)) (- 4))
                   (= (* y 5) (- 2 x)))))
    (and
      i1
      (or (> x y) (= A (< (* 3 x) (+ (- 1) (* 1 (+ x y)))))))))
(check-sat)
(exit)
\end{verbatim}}


\header{QF\_BV}

{\footnotesize
\begin{verbatim}
(set-logic QF_BV)
(declare-fun x () (_ BitVec 32))
(declare-fun y () (_ BitVec 16))
(declare-fun z () (_ BitVec 20))
(assert
  (let ((c1 (= x ((_ sign_extend 12) z))))
   (let ((c2 (= y ((_ extract 18 3) x))))
    (let ((c3 
            (bvslt (concat z (_ bv5 12)) 
              (bvand (bvor (bvxor (bvnot x) ((_ zero_extend 28) #b1111)) 
                                    (concat #xAF02 y))
                    (concat (bvmul ((_ extract 31 16) x) y) 
                            (bvashr (_ bv42 16) #x0001))))))
   (and c1 (xor c2 c3))))))
(check-sat)
(exit)
\end{verbatim}}


\header{QF\_AUFLIA}

{\footnotesize
\begin{verbatim}
(set-logic QF_AUFLIA)
(declare-fun A () (Array Int Int))
(declare-fun x () Int)
(declare-fun y () Int)
(declare-fun P () Bool)
(declare-sort U 0)
(declare-fun f (U) (Array Int Int))
(declare-fun c () U)
(assert
  (let ((fc (f c)))
    (and
      (=> (= A (store fc x 5)) (> (+ (select fc y) (* 4 x)) 0))
      (= P (< (select A (+ 3 y)) (* (- 2) x))))))
(check-sat)
(exit)
\end{verbatim}}


\header{QF\_ABV}

\footnotesize
\begin{verbatim}
(set-logic QF_ABV)
(declare-fun x () (_ BitVec 32))
(declare-fun y () (_ BitVec 16))
(declare-fun z () (_ BitVec 20))
(declare-fun A () (Array (_ BitVec 16) (_ BitVec 32)))
(assert
  (let ((c1 (= ((_ sign_extend 12) z) (select A y)))
        (A2 (store A ((_ extract 15 0) x) x)))
   (let ((c2 (= A A2)))
    (let ((c3 
            (bvslt (concat z (_ bv5 12)) 
              (bvand (bvor (bvxor (bvnot x) 
                                  (select A2 ((_ zero_extend 12) #b1111)))
                                    (concat #xAF02 y))
                    (concat ((_ extract 15 0) 
                                (bvmul x (select (store A y x) #x35FB))) 
                            (bvashr (_ bv42 16) #x0001))))))
   (and c1 (xor c2 c3))))))
(check-sat)
(exit)
\end{verbatim}

\section{Sample benchmark scripts for the application track}

\footnotesize
\begin{verbatim}
(set-option :print-success true)
(set-logic QF_LRA)
(declare-fun c0 () Bool)
(declare-fun E0 () Bool)
(declare-fun f0 () Bool)
(declare-fun f1 () Bool)
(push 1)
(assert 
  (let ((.def_10 (not f0)))
   (let ((.def_9 (not c0)))
    (let ((.def_11 (or .def_9 .def_10)))
     (let ((.def_7 (not f1)))
      (let ((.def_8 (or c0 .def_7)))
       (let ((.def_12 (and .def_8 .def_11)))
 .def_12
)))))))
(check-sat)
(pop 1)
(declare-fun f2 () Bool)
(declare-fun f3 () Bool)
(declare-fun f4 () Bool)
(declare-fun c1 () Bool)
(declare-fun E1 () Bool)
(assert 
 (let ((.def_23 (not f2)))
  (let ((.def_20 (= c0 c1)))
   (let ((.def_22 (or E0 .def_20)))
    (let ((.def_24 (or .def_22 .def_23)))
     (let ((.def_18 (not f4)))
      (let ((.def_19 (or c1 .def_18)))
       (let ((.def_25 (and .def_19 .def_24)))
 .def_25
))))))))
(push 1)
(check-sat)
(assert (and f1 (not f1)))
(check-sat)
(pop 1)
(exit)
\end{verbatim}

\end{document}
