\documentclass[12pt]{article}
\usepackage{times}
\usepackage{fullpage}
\usepackage{url}
\usepackage{epsf}

\newcommand{\akey}[1]{\textbf{#1}}

\begin{document}

\date{}

\title{Satisfiability Modulo Theories Competition (SMT-COMP) 2011: Rules and 
Procedures
}

% [morgan] do our own layout of authors; the four-author layout spacing
% was screwed up...
\def\doauthor#1{{%
  \hsize.5\hsize \advance\hsize by-1cm %
  \def\\{\hss\egroup\hbox to\hsize\bgroup\strut\hss}%
  \vbox{\hbox to\hsize\bgroup\strut\hss#1\hss\egroup}}}%

\def\header#1{\medskip\noindent\textbf{#1}}

\author{
Roberto Bruttomesso\\
Computer Science\\
USI, Lugano (Switzerland)
\and
Morgan Deters \\
Computer Science \\
New York University \\
\and
Alberto Griggio\\
ES Division\\
FBK, Trento (Italy)
}

\maketitle

\def\eg{\textit{e.g.}}
\def\ie{\textit{i.e.}}

\section{Introduction}
\label{sec:intro}

The annual Satisfiability Modulo Theories Competition~(SMT-COMP) is
held to spur advances in SMT solver implementations on benchmark
formulas of practical interest.  Public competitions are a well-known
means of stimulating advancement in software tools.  For example, in
automated reasoning, the CASC and SAT competitions for first-order and
propositional reasoning tools, respectively, have spurred significant
innovation in their fields~\cite{PSS02,leberre+03}.  More information
on the history and motivation for SMT-COMP can be found at the
SMT-COMP web site, \url{www.smtcomp.org}, and in reports on previous
competitions~(\cite{SMTCOMP-2008,BDOS08,SMTCOMP-FMSD,SMTCOMP-JAR}).
SMT-COMP 2011 is affiliated with the 23nd International
Conference on Computer Aided Verification~(CAV).

The rest of this document, updated from the last year's
version\footnote{Earlier versions of this document include contributions from
Clark Barrett, Albert Oliveras, and Aaron Stump.},
describes the rules and competition procedures for SMT-COMP~2011.
The principal change from last year's version is:

\begin{itemize}
\item the introduction of an ``application track,'' based on incremental
      model-checking queries, to promote the integration of SMT solvers
      within actual verification frameworks;
%\item updates to the benchmark selection algorithm, to reduce
%      overwhelming a competition division with a single ``family'' of
%      benchmarks.
% rb 25/3: see later
%\item updates to the difficulty calculation;
%\item a track for parallel solvers, to run before the regular competition.
%\item 5 random ``check'' benchmarks will be generated using Robert
%  Brummayer's SMT fuzzing tool, after all the solvers are uploaded.
%  These will be included in the competition, to help promote attention
%  to the problem of solver correctness (cf.~\cite{brummayer+09}).
\end{itemize}

\section{Entrants}
\label{sec:entrants}

\header{Solver format.} %
An entrant to SMT-COMP is an SMT solver
submitted using the SMT-Exec service.  SMT-Exec enables members of the
SMT research community to run solvers on jobs consisting of benchmarks
from the SMT-LIB benchmark library.  Jobs are run on a 12-node computer
cluster purchased with funds from a National Science Foundation
Computing Research Infrastructure grant.  SMT-Exec is provided free of
charge, but it does require a minimal registration, which verifies an
email address and prevents misuse of the service.  Registered users
may then upload their own solvers to run, or may run public solvers
already uploaded to SMT-Exec.  SMT-Exec provides a variety of tabular
and graphical displays of results of solver executions, for
comparison. For the main (sequential-solver)
% rb 23/3: I guess also application track runs on 
% the same clusters as sequential
and application tracks 
of SMT-COMP 2011, SMT-Exec will be configured so that jobs are run on a 64-bit,
uniprocessor Linux kernel.  For the parallel-solver track, SMT-Exec
will be configured so that jobs are run on a 64-bit, multiprocessor
Linux kernel.\footnote{This year, the competition may take
  advantage of off-site computing resources not normally part of
  SMT-Exec; please stay tuned to the SMT-COMP mailing list for final
  details regarding the effective SMT-COMP 2011 competition cluster.}

For participation in SMT-COMP, a solver must be
uploaded as a ``competition'' solver via the usual SMT-Exec upload
mechanism, or, alternatively, a previously-uploaded solver may be
marked as a ``competition'' solver.  In either case, \emph{uploads
must be marked for competition before the deadline;} uploading a solver
to SMT-Exec is not sufficient for competition entry if it's not marked
as being a competition entrant.  The md5 checksums of competition
submissions will be public immediately after the deadline for competition
entry has passed to ensure transparency, and the submissions themselves
will be made public after the competition.  Source code need not be
provided.  However, in order to encourage sharing of source code, extra
recognition will be given to solvers providing source code distributions
including recognition for the top such solver in each division.  See
SMT-Exec~(\url{www.smtexec.org}) for instructions on uploading solvers, as well
as machine specifications.

\header{System description.} %
As part of their submission via SMT-Exec,
SMT-COMP entrants must also include a short (1--2 pages) description of
the system.  This should include a list of all authors of the system
and their present institutional affiliations.  The programming
language(s) and basic SMT solving approach employed should be
described (\eg, lazy integration of a Nelson-Oppen combination with
SAT, translation to SAT, etc.).  System descriptions are encouraged to
include a URL for a web site for the submitted tool, but this is
optional.  System descriptions must also include a 32-bit unsigned
integer.  These numbers, collected from all submissions, are used
to seed the pseudo-random benchmark selection algorithm, as well
as the benchmark scrambler.

The SMT-Exec upload system will ask for the system description and
the random seed when a solver is marked for competition, so
their inclusion in the uploaded archive itself is optional.

\header{Other systems.} %
As in previous years, due to limitations on
computational resources, the organizers reserve the right not to
accept multiple versions of the same solver (defined as sharing 50\%
or more of its source code).  The organizers reserve the right to
submit their own systems, or other systems of interest, to the
competition.

\header{Wrapper tools.} %
A \emph{wrapper tool} is defined as any tool
which calls an SMT solver not written by the author of the wrapper
tool.  The other solver is called the \emph{wrapped tool}.  There are
several rules governing wrapper tools.  For the purposes of these rules,
multiple versions of a wrapped tool are considered different tools.
The goal of these rules is to require wrapper tools to outperform the
tools they wrap (since otherwise, there is no apparent quantitative
way to argue that the wrapper tool improves upon the wrapped tool).

\begin{itemize}
\item The name of the wrapper tool must end with ``+name'', where name
is the name of the wrapped tool (\eg\ ``Flash+CVC3'' for a tool
wrapping CVC3).

\item If the wrapped tool is from last year's SMT-COMP or earlier, then for
each division entered by the wrapper tool, if the wrapper tool does not place ahead, according to the scoring
rules below, of last year's winner in that division, it will be disqualified
from that division (but not necessarily from the whole competition).

\item If the wrapped tool was released after last year's SMT-COMP,
then the wrapper tool can be entered only if

\begin{itemize}
\item Permission has been given by the author of the wrapped tool

\item The wrapped tool is submitted and entered in every division
      in which the wrapper tool is entered.
\end{itemize}

For each division entered by the wrapper tool, if the wrapper tool
does not place ahead of the wrapped tool in that division, it will be
disqualified from that division.
\end{itemize}

\header{Attendance.} %
As with previous SMT-COMPs, submitters of an SMT-COMP entrant need not 
be physically present at the competition to
participate or win.

\subsection*{Deadlines} %

\paragraph{Main competition track.} %
SMT-COMP entries must be submitted via SMT-Exec by 7pm, Eastern U.S.
time, July~9th, 2011.  At that time the SMT-Exec service will be
closed to the public to prepare for the competition, with the
exception that resubmissions of existing entries will be accepted
until 7pm, Eastern U.S. time, July~11th, 2011.  We strongly encourage
participants to use this two day grace period \emph{only} for the
purpose of fixing any bugs that may be discovered and not for adding
new features as there will be no opportunity to do extensive testing
using SMT-Exec after the original deadline on July~9th.

The versions that are present on SMT-Exec at the conclusion of the
grace period will be the ones used for the competition, and versions
submitted after this time will not be used.  The organizers reserve
the right to start the competition itself at any time after the open
of the New York Stock Exchange on the morning of July~12th.  See
Section~\ref{sec:timeline} below for a full timeline.

\paragraph{Application track.} %
The same deadlines and procedure for submitting to the main track will be used
also for the application track. Submissions to both the application track and
the main competition are independent: participants must submit explicitly to
both events to participate in both, and they may submit different (or
differently configured) solvers to each.  Benchmarks will be scrambled also
for this division, using the same scrambler and seed as the main track.
Entrants should still include a system description, as for the regular
competition.

\paragraph{Parallel solver track.} %
Solvers employing concurrency are invited to participate in a
demonstration that will be run before the regular competition.
Parallel solvers must be submitted by the same July~9th deadline,
with the same two-day grace period for resubmission as in the regular
competition.  Submissions to both the parallel solver track
and the main competition are independent: participants must submit
explicitly to both events to participate in both, and they may submit
different (or differently configured) solvers to each.  Benchmarks may
or may not be scrambled for this division.  Entrants should still
include a system description, as for the regular competition.

\medskip
\noindent
Note that the SMT-COMP organizers are considering ways to include
other (``historical'' or otherwise relevant) solvers in all the competition tracks for
demonstration and comparison.  The organizers reserve the right to
include or exclude such solvers, and to make simple modifications
to historical sequential solvers to (na\"ively) take advantage of
multiple processors 
or to connect them with a parser for the SMT-LIB version 2.0 format
(\eg\ by implementing a wrapper tool).

\section{Execution of Solvers}
\label{sec:exec}

\subsection{Main track and parallel track}
\label{sec:exec:main}

\header{Dates of competition.}
%
We anticipate that the bulk of the main track of the competition will take place during
the course of CAV 2011, from July~14th to~20th.  Results will be
announced in a special session of CAV, on the last day of the
conference, as well as on the SMT-COMP web site.  Intermediate results
will be regularly posted to the SMT-COMP website as the competition
runs.
%
If there are enough competitors to run the parallel track, we intend
to run it before or after the main competition (depending on the
estimated duration of the track).

\header{Input and Output.}
%
Participating solvers must read a single benchmark script (defined
below, not part of the 2.0 standard), presented on its standard input
channel. The script is in the concrete syntax of the SMT-LIB format,
version 2.0.  A benchmark script is essentially just the translation
of a benchmark from the version 1.2 specification.  In more detail, a
\textbf{benchmark script} is just a script where:

\begin{enumerate}
\item The (single) $\akey{set-logic}$ command setting the benchmark's
logic is the first command.
% rb 25/3: added
% \item A (single) \akey{set-option :print-success false} command, sent just
%       after $\akey{set-logic}$. This is to
%       avoid the output ``\texttt{success}'' to interfere with the
%       answer of the solver to the benchmark. 
\item The $\akey{exit}$ command is the last command.
\item There is exactly one $\akey{check-sat}$ command,
following possibly several $\akey{assert}$ commands.
\item There is at most one $\akey{set-info}$ command for \texttt{status}.
\item The formulas in the script belong to the benchmark's logic, with
any free symbols declared in the script.
\item Extra symbols are declared exactly once before any
  use, using $\akey{declare-sort}$
  %, $\akey{define-sort}$,
  %$\akey{declare-fun}$, or $\akey{define-fun}$.  
  or $\akey{declare-fun}$.
  They must be part of the allowed signature expansion for the logic.
  Moreover, all sorts declared with a $\akey{declare-sort}$ command must have zero arity.
\item No other commands besides the ones just mentioned may be used.
\end{enumerate}

% \noindent Note that the final version of the 2.0 format will be frozen
% in early March.  
\noindent The SMT-LIB format specification is publicly
available from the ``Documents'' section of the SMT-LIB
website~\cite{SMT-LIB}.  Solvers will be given formulas just from the
Problem Divisions indicated during their submission to SMT-Exec.
Example benchmark scripts for several Problem Divisions are reported in the Appendix.
Note that they are provided for illustrative purposes only: 
please refer to the SMT-LIB format specification 
and the above definition of benchmark script for the official specification 
of the input format for SMT-COMP.

\medskip
Each SMT-COMP entrant is then expected to attempt to report on its
standard output channel whether the formula is satisfiable
(``\texttt{sat}'', in lowercase, without the quotation marks) or unsatisfiable
(``\texttt{unsat}'').  An entrant may also report ``\texttt{unknown}''
to indicate that it cannot determine satisfiability of the formula.
For more detailed information on the output format, see the
description on the SMT-Exec ``Upload a Solver'' page.

\header{Timeouts.} %
Each SMT-COMP solver will be executed on an
unloaded competition machine for each given formula, up to a fixed
time limit.  The time limit is yet to be determined, but it is
anticipated to be around 20 minutes.  Solvers that take more than this
time limit will be killed.  Solvers are allowed to spawn other
processes.  These will be killed at approximately the same time as the
first started process, using the TreeLimitedRun script, developed for
the CASC competition and available on the SMT-COMP web page.  A
timeout scores the same as if the output is ``\texttt{unknown}''.

\header{Aborts and unparsable output.} %
Solvers which exit before the time
limit without reporting a result (\ie\ due to exhausting memory, crashing,
or producing output other than \texttt{sat}, \texttt{unsat}, or
\texttt{unknown})
will be considered to have aborted. 
An abort scores the same as if the output is ``\texttt{unknown}''. 
% rb 25/3: added
Also, as a further measure to prevent misjudgments of solvers,
the first burst of  ``\texttt{success}'' outputs will be 
ignored\footnote{
Note that SMT-LIBv2 requires to produce a ``\texttt{success}'' answer
after each \akey{set-logic}, \akey{declare-sort}, \akey{declare-fun} and
\akey{assert} command (among others), unless the option
\akey{:print-success} is set to false: ignoring the first burst of
\texttt{success} outputs therefore allows for submitting fully-compliant
solvers without the need of a wrapper script, while still allowing entrants
of SMT-COMP 2010 to run without changes.}.

\header{Persistent state.} %
Solvers are allowed to create and write to
files and directories during the course of an execution, but they are
not allowed to read such files back during later executions.  Any
files written should be put in the directory in which the tool is
started, or in a subdirectory.

\subsection{Application track}
\label{sec:exec:application}

The application track aims at evaluating SMT solvers when interacting
with an external verification framework, \eg, a model
checker. This interaction, ideally, happens by means of an online
communication between the model checker and the solver: the model
checker repeatedly sends queries to the SMT solver, which in turn
answers either sat or unsat.  In this interaction an SMT-solver is
required to accept queries incrementally via its standard input channel.

In order to facilitate the evaluation of the solvers in this track, we
propose to set up a ``simulation'' of the aforementioned interaction. In
particular each benchmark in the application track represents a realistic
communication trace, containing multiple \akey{check-sat} commands (possibly
with corresponding \akey{push 1}/\akey{pop 1} commands), which
is parsed by a {\em trace executor}. The trace executor serves the following purposes:
\begin{itemize}
\item it simulates the online interaction by sending single queries to the SMT solver
      (through stdin);
\item it prevents ``look-ahead'' behaviours of SMT solvers;
\item it records time and answers for each call, possibly aborting the execution
      in case of a wrong answer;
\item it guarantees a fair execution for all solvers by abstracting from any possible
      crash, misbehaviour, etc. that may happen on the model checker side.
\end{itemize}

\header{Input and Output.}
Participating solvers will be connected to a trace executor 
which will incrementally send commands to the standard input channel of the solver
and read responses from the standard output channel of the solver.
The commands will be taken from an \textbf{incremental benchmark script},
which is an SMT-LIB 2.0 script which satisfies points 1., 2., 4.--7. of the definition of benchmark script given in \S\ref{sec:exec:main}, plus the following:

\begin{enumerate}
\item[3'.] There are one or more \akey{check-sat} commands, 
  each preceded by one or more \akey{assert} commands 
  and zero or more \akey{push 1} commands, 
  and followed by zero or more \akey{pop 1} commands.
\end{enumerate}

\noindent
Furthermore, the trace executor will send a single 
\akey{set-option :print-success true} command to the solver before 
sending commands from the incremental benchmark script.

\medskip
Solvers must respond immediately to the commands sent by the trace executor, 
with the answers defined in the SMT-LIB 2.0 format specification: that is,
with a \texttt{success} answer for 
\akey{set-option}, 
\akey{declare-sort}, 
\akey{declare-fun}, 
\akey{assert}, 
\akey{push 1}, 
and \akey{pop 1} 
commands,
and with a \texttt{sat}, \texttt{unsat}, or \texttt{unknown} 
for \akey{check-sat} commands.

\header{Timeouts.}
A time limit is set for the whole execution of each application
benchmark (consisting of multiple queries).

\header{Scoring.}
The score for each benchmark is a pair $\langle n,m\rangle$, with
$n\in[0,N]$ an integral number of points scored for the benchmark,
where $N$ is the number of \akey{check-sat} commands
in the benchmark.  $m\in[0,T]$ is the (real-valued) time is seconds, where $T$ is
the timeout.  The score for the benchmark is initialized with
$\langle0,0\rangle$ and then computed as follows.
\begin{itemize}
\item A correctly-reported \texttt{sat} or \texttt{unsat} answer after
  $s$~seconds (counting from the beginning of this particular
  \akey{check-sat}) contributes $\langle1,s\rangle$ to the running
  score.
\item An answer of \texttt{unknown}, an unexpected answer, a crash, or a memory-out during
  execution of the query, or a benchmark timeout, aborts the execution
  of the benchmark and assigns the current value of the running score
  to the benchmark.  (Recall that there is one timeout for the entire
  benchmark; there are no individual timeouts for queries.)
\item The first incorrect answer to a \akey{check-sat} command has the effect of terminating the
  trace executor, and the returned score for the overall benchmark
  will be $\langle0,0\rangle$, effectively canceling the running score.
\end{itemize}

For example, if a benchmark has 5~\akey{check-sat} commands, and a
timeout of 100~seconds, and a solver solves the first four in
10~seconds each, then times out on the fifth, then the solver's score
is $\langle4,40\rangle$.  If another solver solves each of the first
four in 10~seconds each, and the fifth in another 40~seconds, its
score is $\langle5,80\rangle$.  If a third solver solves the first
four queries in 2~seconds each, but incorrectly answers the fifth, its
score is $\langle0,0\rangle$.

As queries are only presented in order, this scoring system may mean
that relatively ``easier'' queries are hidden behind more difficult
ones located at the middle of the query sequence.

Scores are compared lexicographically---a solver with a higher
$n$-value wins, with the cumulative time only used to break ties.
Benchmarks' scores are summed componentwise to form a solver's total
score for the competition.

\section{Benchmarks and Problem Divisions}
\label{sec:theories}


We expect the problem divisions for SMT-COMP 2011 to include the following
SMT-LIB \emph{logics}.  These logics are specified in SMT-LIB format on the
SMT-LIB web page.  Note that the ``QF\_'' prefix means the division's
formulas are quantifier-free.  However, the organizers reserve the right to add
(remove) divisions if (not) enough benchmarks and solvers exist for a
particular division.


\begin{itemize}
\item QF\_UF: uninterpreted functions.
\item QF\_RDL: real difference logic.
\item QF\_IDL: integer difference logic.
\item QF\_UFIDL: uninterpreted functions and integer difference logic.
\item QF\_UFLIA: uninterpreted functions and linear integer arithmetic.
\item QF\_UFLRA: uninterpreted functions and linear real arithmetic.
\item QF\_UFNRA: uninterpreted functions and nonlinear real arithmetic.
\item QF\_LRA: linear real arithmetic.
\item QF\_LIA: linear integer arithmetic.
\item QF\_NIA: nonlinear integer arithmetic.
\item QF\_AX: arrays with extensionality.
\item QF\_AUFLIA: arrays, uninterpreted functions and linear integer 
                  arithmetic.
\item QF\_BV: fixed-width bitvectors.
\item QF\_AUFBV: arrays, fixed-width bitvectors and uninterpreted
functions.
\item LRA: (quantified) linear real arithmetic.
\item AUFLIA$+p$: (quantified) arrays, uninterpreted functions and 
linear integer arithmetic, patterns included.
\item AUFLIA$-p$: (quantified) arrays, uninterpreted functions and 
linear integer arithmetic, patterns not included.
\item AUFLIRA: (quantified) arrays, uninterpreted functions and 
mixed linear integer and real arithmetic.
\item UFNIA$+p$: (quantified) uninterpreted functions and 
nonlinear integer arithmetic, patterns included.
\item AUFNIRA: (quantified) arrays, uninterpreted functions and 
mixed nonlinear integer and real arithmetic.
\end{itemize}

\subsection{Main and parallel tracks}

\header{Benchmark sources.} %
Benchmark formulas for these divisions
will be drawn from the SMT-LIB library.  Any benchmarks added to
SMT-LIB by the May 15th release (see the timeline in
Section~\ref{sec:timeline}) will be considered eligible.  SMT-COMP
attempts to give preference to benchmarks that are ``real-world,'' in
the sense of coming from or having some intended application outside
SMT.

\header{Benchmark availability.} % A first release of the competition
Benchmarks will be made available within May 15th, 2011.
% A second and almost final release will be available on June 1st. 
No additional
benchmarks will be added after this date, but benchmarks can be
modified or removed to fix possible bugs or other issues, or to adjust their difficulty score (see below). 
The final release that will be used for the competition will be posted on June
30th. The set of selected benchmarks will be published when the
competition begins.

\header{Benchmark demographics.} %
In SMT-LIB, benchmarks are organized according to \emph{families}.  A benchmark
family contains problems that are similar in some significant way.  Typically
they come from the same source or application, or are all output by the same
tool.  \emph{Each top-level subdirectory within a division represents a distinct
family.}  This represents a significant change from
competition years before 2010, where
\emph{terminal} subdirectories delineated families.
%
Each benchmark in SMT-LIB also has a \emph{category}.  There are four possible
categories:
%
\begin{itemize}
\item \emph{check.} These benchmarks are hand-crafted to test whether
  solvers support specific features of each division.  In particular,
  there are checks for integer completeness (\ie\ benchmarks that are
  satisfiable under the reals but not under the integers) and big
  number support (\ie\ benchmarks that are likely to fail if integers
  cannot be represented beyond some maximum value, such as
  $2^{31}-1$).

  \noindent%\textbf{New for 2010:} 
  Using the same random seed as
  for benchmark selection and scrambling, 5 ``check'' benchmarks will
  be randomly generated using Robert Brummayer's SMT fuzzing
  tool~\cite{brummayer+09}.  The (exact version of this) tool will be
  publicly available from the SMT-COMP~2011 web site before the
  competition.  Since these benchmarks are generated after the random
  seed is fixed, they cannot be known in advance to any competitors,
  including the organizers.  The rationale for including these
  benchmarks is to try to take a step towards stronger certification
  that solvers are correct.  In future years, we envision SMT-COMP
  requiring that solvers pass some kind of pre-qualifying round based
  on SMT fuzzing.  The inclusion of these fuzzing benchmarks that are
  not known in advance is intended to encourage (without requring)
  solver implementors to test their solvers using fuzzing or similar
  bug-discovery techniques.
\item \emph{industrial.} These benchmarks come from some real application
      and are produced by tools such as bounded model checkers, static analyzers, extended
      static checkers, etc.
\item \emph{random.} These benchmarks are randomly generated.
\item \emph{crafted.} This category is for all other benchmarks.  Usually,
  benchmarks in this category are designed to be particularly difficult or to
  test a specific feature of the logic.
\end{itemize}

\header{Benchmark selection.} %
Before the selection process, each benchmark will be assigned a
\emph{difficulty:} a number between 0.0 and 5.0 inclusive.  The
difficulty for a particular benchmark will be assigned by running SMT
solvers from the 2010 competition that finished in good
standing and using the formula:

\[\mathrm{difficulty} = {5\cdot\ln\!\left(1+A^2\right) \over \ln\!\left(1+30^2\right)}\]

\noindent
where $A$ is the average time for the solvers to correctly solve the
instance (in minutes).  This computation of difficulties
replaces a simpler formulation in earlier SMT-COMPs that didn't take
into account the time solvers take.  This calculation of difficulty
recognizes that problems requiring more time by many solvers are
more difficult problems.  The logarithm is used to
mark a larger change in difficulty (given a corresponding increase in
solver average time) at smaller time scales than at higher ones (if $A=1$,
difficulty is 0.5; at $A=1.7$, difficulty is 1.0; but a difficulty of 2.0
requires that $A=4$); the square is used to flatten out this curve sightly
at the low end.  Figure~\ref{difficultyplot} shows this curve.

\begin{figure}
  \centerline{\epsffile{difficultyplot.eps}}
  \caption{The shape of the difficulty assignment curve.}
  \label{difficultyplot}
\end{figure}

When 5 or more solvers are used for the
calculation, two times are dropped from the calculation of the average
(one at the maximum and one at the minimum).  Solvers giving an
incorrect answer are not counted in the average; solvers crashing,
timing out, or giving an unknown result are considered as taking 30
minutes (which, with the above formula, pulls the difficulty
toward~5.0).  If there are available solvers, but no average is
defined under these rules, the difficulty shall be 5.0.  For new
divisions, where there are no available solvers to compute the
difficulty, the difficulty will be computed using whatever means are
available to the organizers for that purpose.

The following scheme will be used to choose competition benchmarks
within each division.  Unknown-status benchmarks from SMT-LIB are
considered ineligible for competition and are not used.
%
Additionally, certain benchmarks deemed inappropriate for competition
may be cut by the organizers, and in some cases, representative subsets
of benchmarks from a certain source may be included for the selection
instead of all benchmarks from that source.
%
The selection
is implemented by our benchmark selection tool, source for which will
be available at \url{www.smtcomp.org}.

\begin{enumerate}

\item \textbf{Check benchmarks included.} %
  All benchmarks in category \emph{check} are included.

\item \textbf{Retire very easy benchmarks.} %
  The most difficult~300 non-check non-unknown benchmarks in each
  division are always included, together with all benchmarks on which
  at least one 2010 solver required more than 5~seconds.  \emph{This
    is intended to have the effect of retiring ``very easy''
    benchmarks that were solved by every 2010 solver in less
    than 5~seconds, \emph{unless} doing so reduces the pool of
    benchmarks for the division to less than~300.}

\item \textbf{Retire inappropriate benchmarks.} %
  The competition organizers will remove from the eligibility pool
  certain SMT-LIB benchmarks that are inappropriate or uninteresting
  for competition, or cut the size of certain benchmark families to
  avoid their over-representation.

\item\label{step:pool} \textbf{Division selection pools created.} %
  For non-\emph{check} benchmarks, selection pools are created.
  For benchmark families with $\leq200$ eligible,
  non-\emph{check} benchmarks, all are added to this pool;
  otherwise, 200 such benchmarks are added to the pool with the
  following distribution:
%
  \begin{itemize}
  \item 20 with solution \textbf{sat} and difficulty on $\left[0,1\right]$
  \item 20 with solution \textbf{sat} and difficulty on $\left(1,2\right]$
  \item 20 with solution \textbf{sat} and difficulty on $\left(2,3\right]$
  \item 20 with solution \textbf{sat} and difficulty on $\left(3,4\right]$
  \item 20 with solution \textbf{sat} and difficulty on $\left(4,5\right]$
  \item 20 with solution \textbf{unsat} and difficulty on $\left[0,1\right]$
  \item 20 with solution \textbf{unsat} and difficulty on $\left(1,2\right]$
  \item 20 with solution \textbf{unsat} and difficulty on $\left(2,3\right]$
  \item 20 with solution \textbf{unsat} and difficulty on $\left(3,4\right]$
  \item 20 with solution \textbf{unsat} and difficulty on $\left(4,5\right]$
  \end{itemize}
%
  If 20 are not available in one of these subdivisions, all that are
  available are added, and remaining slots are reallocated to the
  others.  This process is iterated so that it is guaranteed that 200
  benchmarks from the benchmark family are in the selection pool, in
  equal numbers from each subdivision, so far as possible.
  (In cases where \eg\ there are only two available slots and
  they can be allocated to one of three subdivisions, they are allocated
  randomly but are guaranteed to be allocated to \emph{distinct}
  subdivisions.)

\item\textbf{Category slot allocation.} %
  Next, 200 slots are allocated for the division as follows:\footnote{The
  number ``200'' is a guideline, and is expected to be used.  However, the
  SMT-COMP organizers reserve the right to reduce this to an appropriate
  value to ensure a timely end to the competition, and may do so on a
  per-division basis.  The category allotments, etc., will remain the same
  proportion of the total.}
%
  \begin{itemize}
  \item 170 from category \emph{industrial}
  \item 20 from category \emph{crafted}
  \item 10 from category \emph{random}
  \end{itemize}
%
  If there are fewer than 20 (respectively, 10) \emph{crafted} or
  \emph{random} benchmarks in the division pool, more
  \emph{industrial} slots are allocated to make 200 total for the
  division.  If there are too few \emph{industrial} benchmarks in the
  division pool, more \emph{crafted} slots are allocated to make 200
  total for the division.  (In no division are there not enough of
  both industrial and crafted benchmarks.)

\item\textbf{Category subdivision slot allocation.} %
  For each category, given that it has $n$ slots allocated to it,
  the slot allocation is further subdivided as follows:
%
  \begin{itemize}
  \item $\left\lfloor n\over10\right\rfloor$ slots for solution \textbf{sat} with difficulty on $\left[0,1\right]$
  \item $\left\lfloor n\over10\right\rfloor$ slots for solution \textbf{sat} with difficulty on $\left(1,2\right]$
  \item $\left\lfloor n\over10\right\rfloor$ slots for solution \textbf{sat} with difficulty on $\left(2,3\right]$
  \item $\left\lfloor n\over10\right\rfloor$ slots for solution \textbf{sat} with difficulty on $\left(3,4\right]$
  \item $\left\lfloor n\over10\right\rfloor$ slots for solution \textbf{sat} with difficulty on $\left(4,5\right]$
  \item $\left\lfloor n\over10\right\rfloor$ slots for solution \textbf{unsat} with difficulty on $\left[0,1\right]$
  \item $\left\lfloor n\over10\right\rfloor$ slots for solution \textbf{unsat} with difficulty on $\left(1,2\right]$
  \item $\left\lfloor n\over10\right\rfloor$ slots for solution \textbf{unsat} with difficulty on $\left(2,3\right]$
  \item $\left\lfloor n\over10\right\rfloor$ slots for solution \textbf{unsat} with difficulty on $\left(3,4\right]$
  \item $\left\lfloor n\over10\right\rfloor$ slots for solution \textbf{unsat} with difficulty on $\left(4,5\right]$
  \end{itemize}
%
  Remaining slots are allocated randomly to distinct subdivisions.
  If there aren't enough benchmarks in the pool meeting one or more of
  the above subdivision requirements for the category, the subdivision
  allocation is reduced to the number available in the pool that meet
  the requirements.  To make up the full category allotment, remaining
  slots are allocated equally to subdivisions with enough benchmarks
  in the pool meeting their requirements.  This process ensures that
  all slots can be filled with benchmarks from the pool.

\item\textbf{Benchmark selection.} %
  Benchmarks from the pool are assigned randomly to slots.
\end{enumerate}
%
In the end, up to 200 non-\emph{check} benchmarks per division are
included in the competition, together with all the \emph{check}
benchmarks.  Some divisions may have fewer than 200 non-\emph{check}
benchmarks, in which case all of them are included using this selection
scheme.

The main purpose of the algorithm above is to have a balanced and complete set
of benchmarks.  The one built-in bias is towards industrial rather than crafted
or random benchmarks.  This reflects a desire by the organizers and agreed upon
by the SMT community to emphasize problems that come from real applications.

Pseudo-random numbers will be generated using the standard C library
function \texttt{random()}, seeded (using \texttt{srandom()}) with the
sum, modulo $2^{30}$, of the numbers provided in the system
descriptions (see Section~\ref{sec:entrants} above) by all SMT-COMP
entrants other than the organizers.  Additionally, the integer part of
the opening value of the New York Stock Exchange Composite Index on
July~12th
will be added to the other seeding values.  This helps provide transparency,
by guaranteeing that the organizers cannot manipulate the seed in
favor of their submitted solvers.  Benchmarks will also be slightly
scrambled before the competition, using a simple benchmark scrambler %the same scrambler as last year,
seeded with the same seed as the benchmark selector.  Both the
scrambler and benchmark selector will be publicly available before the
competition.  Naturally, solvers must not rely on previously
determined identifying syntactic characteristics of competition
benchmarks in testing satisfiability (violation of this is considered
cheating).

\subsection{Application track}

\header{Benchmark sources.} %
Benchmarks for the application track will be collected by the SMT-COMP organizers.
Any benchmark available to the organizers by May 15th (see the timeline in
Section~\ref{sec:timeline}) will be considered eligible.

\header{Benchmark availability.} 
A first release of the application track
benchmarks will be made available within March 31st, 2011.
More benchmarks can be collected, until May 15th.
No additional
benchmarks will be added after this date, but benchmarks can be
modified or removed to fix possible bugs or other issues. 
The final release that will be used for the competition will be posted on 
% rb 23/3: changed from June 30th to agree with sec 6.
June 15th.

\header{Benchmark demographics and selection.} 
All the available benchmarks will be used for the competition. 
For this first year of the application track, no difficulty will be assigned to the benchmarks.
Benchmarks will be slightly scrambled before the competition, using the same scrambler and 
random seed as the main track. 
% rb 25/3: added
For this year, the selection algorithm for the application track will be chosen after the
deadline of May 15th, when the exact demographic of the benchmarks (collected with a usual
``call for benchmarks'') will be available.

\section{Judging and Scoring}
\label{sec:judging}

\begin{figure}
\begin{center}
\begin{tabular}{|l|c|c|}
\hline \textbf{Reported} & \textbf{Correct?} &
\textbf{Point/penalty}\\ \hline \texttt{unsat} & yes & +1 \\ \hline
\texttt{unsat} & no & -8 \\ \hline \texttt{sat} & yes & +1 \\ \hline
\texttt{sat} & no & -8 \\ \hline \texttt{unknown} & n.a. & 0 \\ \hline
\emph{timeout} & n.a. & 0 \\ \hline
\emph{abort} & n.a. & 0 \\ \hline
\end{tabular}
\end{center}
\caption{\label{fig:points}Points and Penalties for the main and parallel tracks.}
\end{figure}

Winners in each Problem Division for which there are at least three
entrants from distinct research groups competing will be taken to be
those with the highest score. In addition to recognizing the overall
winner in each division, the top solver that provides its source code will also be
recognized in each division.  
For Problem Divisions with fewer than three
entrants, the results will be reported but no winner officially
declared.

For the main and parallel tracks, the score will be computed according to the system of points and
penalties in Figure~\ref{fig:points}. 
As indicated, correct answers
are awarded a positive number of points, while incorrect answers are
penalized by assigning a negative number of points.  Timeouts, aborts,
and reports of \texttt{unknown} are awarded zero points.
%
In the event of a tie
in total number of points, the solver with the lowest average CPU time
on formulas for which it gave a correct answer will be
considered the winner.  

For the application track, the score will be computed according to the procedure described in \S\ref{sec:exec:application}.

\penalty0

\section{Timeline}
\label{sec:timeline}

\nobreak
\vbox{% no page break here, please!
\begin{description}
\item[March 31] First version of the benchmark library for the application track posted for comment.
  First version of the benchmark scrambler, benchmark selector and trace exector made available.
\item[April 30] Final version of the benchmark scrambler, benchmark selector and trace exector made available.
\item[May 15] No new benchmarks can be added after this date, but
  problems with existing benchmarks may be fixed.
\item[June 15] Benchmark libraries are frozen.
\item[July 9 (7pm ET)] Solvers due via SMT-Exec (for
  all tracks), including system
  descriptions and magic numbers for benchmark scrambling.
\item[July 11 (7pm ET)] Final versions due, fixing any last-minute
 bugs (this marks the end of the two-day grace period for
 submissions).
\item[July 12] Opening value of NYSE Composite Index used to complete random seed.
\item[July 14--20] Anticipated dates for SMT-COMP 2011.
\end{description}}

\section{Mailing List}
\label{sec:ps}

Interested parties should subscribe to the SMT-COMP mailing list, a
link to which is found at \url{www.smtcomp.org}.  Important
late-breaking news and any necessary clarifications and edits to these
rules will be announced there, and it is the primary way that such
announcements will be communicated.

\bibliographystyle{plain}
\bibliography{biblio}

\appendix
\section{Sample benchmark scripts for the main track}

\header{QF\_UF}

{\footnotesize
\begin{verbatim}
(set-logic QF_UF)
(set-info :status sat)
(declare-sort U 0)
(declare-fun f (U) U)
(declare-fun g (U) U)
(declare-fun A () Bool)
(declare-fun x () U)
(declare-fun y () U)
(assert
(let ((fx (f x))
      (cls1 (or A (= x y))))
  (and cls1 (distinct fx (g y)))))
(check-sat)
(exit)
\end{verbatim}}


\header{QF\_LRA}

{\footnotesize
\begin{verbatim}
(set-logic QF_LRA)
(declare-fun x () Real)
(declare-fun y () Real)
(declare-fun A () Bool)
(assert
  (let ((i1 (ite A (<= (+ (* 2.0 x) (* (/ 1 3) y)) (- 4))
                   (= (* y 1.5) (- 2 x)))))
    (and
      i1
      (or (> x y) (= A (< (* 3 x) (+ (- 1) (* (/ 1 5) (+ x y)))))))))
(check-sat)
(exit)
\end{verbatim}}


\header{QF\_LIA}

{\footnotesize
\begin{verbatim}
(set-logic QF_LIA)
(declare-fun x () Int)
(declare-fun y () Int)
(declare-fun A () Bool)
(assert
  (let ((i1 (ite A (<= (+ (* 2 x) (* (- 1) y)) (- 4))
                   (= (* y 5) (- 2 x)))))
    (and
      i1
      (or (> x y) (= A (< (* 3 x) (+ (- 1) (* 1 (+ x y)))))))))
(check-sat)
(exit)
\end{verbatim}}


\header{QF\_BV}

{\footnotesize
\begin{verbatim}
(set-logic QF_BV)
(declare-fun x () (_ BitVec 32))
(declare-fun y () (_ BitVec 16))
(declare-fun z () (_ BitVec 20))
(assert
  (let ((c1 (= x ((_ sign_extend 12) z))))
   (let ((c2 (= y ((_ extract 18 3) x))))
    (let ((c3 
            (bvslt (concat z (_ bv5 12)) 
              (bvand (bvor (bvxor (bvnot x) ((_ zero_extend 28) #b1111)) 
                                    (concat #xAF02 y))
                    (concat (bvmul ((_ extract 31 16) x) y) 
                            (bvashr (_ bv42 16) #x0001))))))
   (and c1 (xor c2 c3))))))
(check-sat)
(exit)
\end{verbatim}}


\header{QF\_AUFLIA}

{\footnotesize
\begin{verbatim}
(set-logic QF_AUFLIA)
(declare-fun A () (Array Int Int))
(declare-fun x () Int)
(declare-fun y () Int)
(declare-fun P () Bool)
(declare-sort U 0)
(declare-fun f (U) (Array Int Int))
(declare-fun c () U)
(assert
  (let ((fc (f c)))
    (and
      (=> (= A (store fc x 5)) (> (+ (select fc y) (* 4 x)) 0))
      (= P (< (select A (+ 3 y)) (* (- 2) x))))))
(check-sat)
(exit)
\end{verbatim}}


\header{QF\_ABV}

\footnotesize
\begin{verbatim}
(set-logic QF_ABV)
(declare-fun x () (_ BitVec 32))
(declare-fun y () (_ BitVec 16))
(declare-fun z () (_ BitVec 20))
(declare-fun A () (Array (_ BitVec 16) (_ BitVec 32)))
(assert
  (let ((c1 (= ((_ sign_extend 12) z) (select A y)))
        (A2 (store A ((_ extract 15 0) x) x)))
   (let ((c2 (= A A2)))
    (let ((c3 
            (bvslt (concat z (_ bv5 12)) 
              (bvand (bvor (bvxor (bvnot x) 
                                  (select A2 ((_ zero_extend 12) #b1111)))
                                    (concat #xAF02 y))
                    (concat ((_ extract 15 0) 
                                (bvmul x (select (store A y x) #x35FB))) 
                            (bvashr (_ bv42 16) #x0001))))))
   (and c1 (xor c2 c3))))))
(check-sat)
(exit)
\end{verbatim}

\section{Sample benchmark scripts for the application track}

\footnotesize
\begin{verbatim}
(set-option :print-success true)
(set-logic QF_LRA)
(declare-fun c0 () Bool)
(declare-fun E0 () Bool)
(declare-fun f0 () Bool)
(declare-fun f1 () Bool)
(push 1)
(assert 
  (let ((.def_10 (not f0)))
   (let ((.def_9 (not c0)))
    (let ((.def_11 (or .def_9 .def_10)))
     (let ((.def_7 (not f1)))
      (let ((.def_8 (or c0 .def_7)))
       (let ((.def_12 (and .def_8 .def_11)))
 .def_12
)))))))
(check-sat)
(pop 1)
(declare-fun f2 () Bool)
(declare-fun f3 () Bool)
(declare-fun f4 () Bool)
(declare-fun c1 () Bool)
(declare-fun E1 () Bool)
(assert 
 (let ((.def_23 (not f2)))
  (let ((.def_20 (= c0 c1)))
   (let ((.def_22 (or E0 .def_20)))
    (let ((.def_24 (or .def_22 .def_23)))
     (let ((.def_18 (not f4)))
      (let ((.def_19 (or c1 .def_18)))
       (let ((.def_25 (and .def_19 .def_24)))
 .def_25
))))))))
(push 1)
(check-sat)
(assert (and f1 (not f1)))
(check-sat)
(pop 1)
(exit)
\end{verbatim}

\end{document}
