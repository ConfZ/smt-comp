{\rtf1\ansi\ansicpg1252\deff0\deflang1033{\fonttbl{\f0\fnil\fcharset0 Calibri;}}
{\colortbl ;\red0\green0\blue255;}
{\*\generator Msftedit 5.41.21.2510;}\viewkind4\uc1\pard\sa200\sl276\slmult1\lang9\f0\fs22 From: Matti Jarvisalo <matti.jarvisalo@cs.helsinki.fi>\par
To: Tjark Weber <tjark.weber@it.uu.se>\par
Cc: David R. Cok <cok@frontiernet.net>, David D\'e9harbe\par
<david@dimap.ufrn.br>, marijn@cs.utexas.edu\par
Subject: [JSAT] Editor Decision\par
Date: Sun, 7 Jun 2015 17:09:47 +0200\par
\par
Dear authors,\par
\par
Thank you for submitting the manuscript "The 2014 SMT Competition" for\par
publication in the forthcoming Journal of Satisfiability, Boolean Modeling\par
and Computation Special Issue on SAT 2014 Competitions and Evaluations. We\par
apologize for the delay due to unexpected delays in the reviewing process.\par
The reviews for the manuscript are attached.\par
\par
The reviewers agree that the paper has potential. However, they also\par
identify several possibilities for greatly improving the article. Among\par
these are:\par
\par
1. There is a lack of depth in analyzing the results of the competition,\par
which should go beyond merely observing the rankings. Essentially, a\par
full-length competition article should provide in-depth analysis of the\par
results, so that the reader can learn more than what can be learned by\par
simply observing cactus plots and/or tables representing overviews of\par
rankings and numbers of instances solved.\par
\par
2. Design choices made in organizing the competition need to discussed,\par
providing justifications for the choices and adding discussion of the\par
possible influence of the choices on the competition results.\par
\par
3. The overall presentation requires major improvements.\par
\par
While we cannot accept the article in its current form for publication, we\par
would be happy to consider a revised manuscript addressing the reviewers'\par
concerns. Thus, we encourage you to resubmit the paper. Should you choose to\par
revise the submission, the revised version is due August 7, 2015. The\par
revision will sent out for a second review. Please notice that we can only\par
allow one re-submission to this special issue.\par
\par
When preparing the revised version please prepare detailed letter explaining\par
how you addressed the reviewers' concerns, and include this letter in the\par
revised paper pdf.\par
\par
\par
Yours sincerely,\par
Marijn Heule and Matti Jarvisalo\par
Special Issue Editors\par
\par
\par
REVIEWER A\par
4: ACCEPT AFTER MINOR REVISIONS\par
\par
This article is a very nice report of the 2014 SMT Competition. (Almost)\par
everything that I would expect of such a report is there. The paper is\par
easily readable and very well-written.\par
\par
The only thing that I was missing is whether there have been any\par
\'84surprises\ldblquote  in the results. Is there a category in which particular\par
progress could be recorded? What are solver implementers currently\par
concentrating on? A short discussion on this topic would, in my opinion,\par
nicely round off the paper.\par
\par
A minor issue: Readers which are not so familiar with SMT would perhaps\par
profit from a short description of the different logics (e.g. AUFLIA, BV,\par
NRA, \'85). As the logics\rquote  names are quite systematic, a few sentences or a\par
small table should suffice.\par
\par
- p. 2, 2nd par.: Reference [6] is a bit old, describing the 2003 SAT\par
Competition. Perhaps you could use something more up-to-date or add a link\par
to the SAT Competitons' web page ({\field{\*\fldinst{HYPERLINK "http://www.satcompetition.org"}}{\fldrslt{\ul\cf1 http://www.satcompetition.org}}}\f0\fs22 )\par
- p. 2, 1st par. of sec. 2: "...and, perhaps, some constraints on the kinds\par
of expressions in that theory.": that seems a bit vague; perhaps you could\par
give an example (e.g. linear arithmetic)?\par
- p. 2, 1st par. of sec. 2: "Each problem has a set of constant or function\par
symbols;...": This sentence seems to suggest that variables are not allowed,\par
and only QF fragments of PL1 are considered. However, the example on the\par
next page is using quantifiers and variables.\par
- p. 4, fig. 1: Fractional numbers on the y-axis don't make too much sense.\par
Perhaps you could re-scale or drop the number 0.1 from the y-axis.\par
- p. 4, sec. 3: "The organizers still maintain the value...": perhaps change\par
to "The organizers still appreciate the value..."?\par
- p. 5, sec. 4: The timeline is quite detailed. I am not sure if all this\par
information (especially the exact dates) is of interest to the reader.\par
- p. 7, last par., on \'84scrambling\ldblquote : The problem of highly deviating\par
runtimes on scrambled instances has been discussed in the SAT community some\par
time ago. The 2002 and 2003 SAT Competitions used scrambling, but all\par
refrained from using it, in particular on industrial instances. A discussion\par
on this can be found, e.g., in your reference [6] (Sec. 5.4, 2nd par.).\par
Perhaps you could make a reference  to this discussion?\par
- p. 11, \'84Selection of benchmarks\ldblquote : You describe to some detail a\par
selection process (based on difficulty), but, if I understand correctly,\par
didn\rquote t use it. This fact should be clarified.\par
- p. 12, last par. of sec. 7: I found this paragraph a bit hard to\par
understand. Perhaps you could give an example? (Or drop that paragraph.)\par
- p. 12, sec. 8, 2nd par.: Do the machines really have 129 resp. 258 GB of\par
memory (and not 128/256)? Moreover, it would be nice to know the full model\par
number of the processor (e.g., Intel Xeon E6520), as comparing CPU speeds\par
based on clock frequency becomes less significant (with turbo modes etc.).\par
- p. 14, fig. 2: (minor issue) Perhaps you can re-scale the x-axis to have\par
equal padding on both sides of the graph.\par
- p. 16, first line: On first reading, it wasn\rquote t clear to me if e_i also\par
includes timeouts. Perhaps you can clarify this, adding something like\par
\'84(not including timeouts).\par
- p. 16/17, discussion on metrics: Perhaps you could shortly mention metrics\par
that are used in other competitions, e.g. SV-COMP\par
({\field{\*\fldinst{HYPERLINK "http://sv-comp.sosy-lab.org"}}{\fldrslt{\ul\cf1 http://sv-comp.sosy-lab.org}}}\f0\fs22 )?\par
- p. 19, middle: \'84Resolve the definition of divide-by-zero\'85\ldblquote : Could\par
you state what the current definition in SMT-LIB is?\par
- p. 23, table 1: How is this table ordered? If looking for a particular\par
solver, it would be easier to find it if the table was sorted by solver name\par
(as it is done, e.g., in table 4).\par
- p. 32, table 11: Could that table be ordered, too?\par
\par
\par
REVIEWER B\par
3: POSSIBLE ACCEPT AFTER MAJOR REVISIONS\par
\par
The paper describes the SMT competition providing an in depth description of\par
the organization process,\par
and a less detailed description of the results.\par
The contribution of the paper --as it is in general the organization of a\par
competition-- is substantial,\par
but there are some presentation issues that can be solved in an additional\par
round of review.\par
\par
In particular the main presentation issues are:\par
\par
1- Paper structure needs revision to reduce redundancy\par
2- The effect of some design choices should be better discussed/motivated\par
3- The analysis of the results can be more detailed and placed in one\par
section\par
\par
This is a more detailed list of suggestions that the authors may want to\par
take into account for improving the paper:\par
\par
- Paper content should be made more homogeneous and less redundant (consider\par
merging some sections)\par
 -- There are repetitions of the same concepts in several sections\par
    (What is SMT, Starexec usage issues, comments about the increase of the\par
time limit, SMTLib language,\par
    aggregate data on benchmarks, and organization details such as the\par
number of divisions/partecipans to mention a few)\par
 -- The difficulties encountered adopting Starexec seem to be a dominant\par
topic in the paper,\par
    maybe the lessons learned could be summarized only in the specific\par
section 8\par
 -- The flow of the paper can be made more linear\par
   -- Consider to merge Section 1, part of section 2, with some historical\par
notes and goals in the introduction.\par
   -- Consider to separate goals from actions, moving organization details\par
from Section 3 to another section\par
   -- Section 5 is basically anticipating all the information one can find\par
later, and in my view is the main source of redundancy,\par
      Consider moving Section 5 just before the conclusion, just to detail\par
the procedure and the timeline of the various steps,\par
      at the same time what was already detailed before can be dropped from\par
this section.\par
   -- Section 10 FLoC Olympic Games should be called something like\par
"Scoring" and should be presented before commenting the results.\par
      Actually I would suggest move the comment on results reported in\par
section 10 to the results section.\par
\par
 -- Section 4 can be dropped, the effort in terms of days can be mentioned\par
in aggregate form in the intro of in the competition procedure section,\par
punctual information can be given in terms of a link to the competition\par
website\par
\par
- The effect of some design choices and missing comparisons are not\par
explained with rigor\par
 -- Why do you stress the choice of a different time-out?\par
    Once you change the hardware, even if you fix the timeout, you are\par
actually changing the budget of allowed CPU work.\par
    BTW, with larger timeouts (once the machine it is fixed) one can only\par
expect that less optimized systems\par
    perform better in terms of number of solved instances.\par
    Thus, in my view any reasonable choice of timeout is worth, baring in\par
mind that different choices lead to different results.\par
 -- About the impossibility to compare with past editions.\par
    You could either run the system that won last competition on the new\par
benchmarks and/or, vice versa,\par
    you could run the winner of the latest competition on the old benchmarks\par
to provide at least an idea of the progress.\par
    I'm obviously assuming that this is done on the same machine (non\par
necessarily starexec) with a fair choice of time and memory limits.\par
    BTW, a comparison in terms of the (possible increase in the) number of\par
supported language features by old systems could be improved,\par
    maybe elaborating "History" paragraph in Section 6.\par
 -- You can consider to comment on the benchmark selection criteria adopted\par
in other competitions, discussing why the selected one was considered worth\par
by the SMT community.\par
\par
- The analysis of the results needs improvements\par
  -- What is indicated by "the effect of timeout" seems to be improper, this\par
analysis is no more than the usual performance analysis\par
     that is customarily done by means of a cactus plot, which provides an\par
intuitive graphic representation of the same data.\par
  -- After reading your mention of the winners in the main text,\par
     I felt alone with a table summarizing the overall results,\par
     then more details came later on a section called FLOC Olympic Games.\par
  -- a good information for a reader would be to get some general directions\par
on what is the system to consider for a specific problem/division without\par
having to study the provided tables\par
\par
Thus I suggest to spend some time putting all the results in the same\par
section and\par
discussing a little more in the main text the tables so to provide the\par
reader with a lesson learned or\par
some additional detail on the performance of participants\par
(e.g., one can discuss the performance of participants on different\par
divisions/benchmark, or maybe can indicate\par
whether some specific evaluation technique or system resulted better than\par
another in a specific division/kind of benchmark, etc.)\par
\par
A list of punctual and minor comments follows.\par
\par
Page 1:\par
"The SAT-solving decision problem"\par
 -SAT is a decision problem, solving is the act of finding a solution.\par
\par
Page 1:\par
"(SMT). The"\par
 -Can you put a citation to a seminal work introducing SMT?\par
\par
Page 2:\par
"skipping only 2013; in that year an evaluation [14] was performed, rather\par
than a competition."\par
 -Can you elaborate on the difference?\par
\par
Page 2:\par
"2. Context of the Competition"\par
 -I would rename this section, something like the SMT-Lib language.\par
\par
Page 2:\par
"The SMT Competition is a competition among SMT solvers on a set of\par
benchmark logic problems expressed in the SMT-LIB [3] language. Each\par
benchmark problem [...] Each combination of underlying theories and language\par
constraints is a logic."\par
 -This part is somehow a repetition of what stated in the introduction.\par
    Why not presenting a more complete overview of the SMT-Lib standard\par
language and move the details about benchmarks in the section discussing the\par
selection of benchmarks?\par
\par
Page 3:\par
"SMT differs from the CASC competition [24] in directly addressing sorted\par
logics. SMT also focuses on fragments of first-order logic that are\par
decidable. For example, a subset of the benchmarks are problems in an\par
integer-difference logic, for which there are specific decision procedures.\par
The competition among tools is to create very efficient implementations of a\par
breadth of decision procedures. Accordingly, SMT solvers have historically\par
not handled logics with quantified expressions, though several solvers now\par
do so."\par
 -Why not collecting differences with related competitions in a dedicated\par
section?\par
\par
Page 4:\par
"\bullet  A significant change in 2014 was to increase the time-out limit for a\par
solver processing a given benchmark from 25 minutes (in 2012) to 40\par
minutes."\par
 -How is his related to hardware speed?\par
A timeout of 10 minutes is equivalent to a timeout of 20 minutes in a\par
machine that is twice faster.\par
The reason for this choice should be better motivated.\par
\par
Page 5:\par
"4. Competition Timeline"\par
 -I find this section not very informative.\par
\par
Page 5:\par
"all benchmarks were run"\par
 -This is not related to the paragraph title.\par
\par
Page 6:\par
"Though the memory limit was fairly generous, a small number (202) of jobs\par
were killed during the competition because of excessive memory use."\par
 -This is not in contrast with having a generous memory limit, I would\par
have expected many memory outs in case of restrictive memory limits.\par
\par
Page 6:\par
"During the testing prior"\par
 -And prior\par
\par
Page 11:\par
"The SMT-LIB language."\par
 -This paragraph sounds like a repetition.\par
\par
Page 13:\par
"9.1 Main track results"\par
 -I see no comment on the results, nor discussion about performance of\par
competing solvers\par
\par
Page 13:\par
"competition,"\par
 -Remove\par
\par
Page 13:\par
"A first question is how many additional problems are solved if the timeout\par
is lengthened."\par
 -Can you be more precise discussing the effect solver wise ?\par
\par
Page 13:\par
"This figure shows the number of benchmarks solved,"\par
 -I assume by some of the participants... or by the winner?\par
\par
Page 14:\par
"9.2 Application (incremental) track results"\par
 -What is the effect of the timeout in this application track?\par
This aspect is underdeveloped, can you make more clear why it is so in the\par
text.\par
\par
Page 16:\par
"Second, if there is a tie in the number of errors and correctly solved\par
problems, the total time taken on the correctly solved problems is used as\par
the tie-breaker;"\par
 -What if two solvers solve different subsets of instances ?\par
\par
Page 16:\par
"Only competitive divisions were included in the scoring. For determining\par
medals, a division is competitive if there are at least two officially\par
registered, participating solvers from different teams"\par
 -Isn't this in contrast with the goal of broadening language support?\par
\par
Page 16:\par
"The log scaling of the scores for each division is a somewhat arbitrary"\par
 -I do not understand the effect of this log, please provide an intuition\par
about its influence on the results.\par
\par
REVIEWER C\par
4: ACCEPT AFTER MINOR REVISIONS\par
\par
This article summarizes the preparation, the organizational aspects and the\par
runoff of the 2014 edition of the SMT competition, SMT-COMP 2014.\par
The authors are the organizers of SMT-COMP 2014.\par
The authors report on what challenges had to be addressed before the\par
competition in order to get the right participation of competition\par
attendants.\par
They also describe what considerations had to be taken in order to make\par
SMT-COMP 2014 running with appropriate benchmark suite on a completely new\par
platform.\par
In addition to SMT-COMP-specific organizational aspects, the authors also\par
had to take into account the requirements of the FLoC Olympic Games 2014\par
event,\par
and make sure that the competition is attractive not only for attentants but\par
for a wider audience.\par
I found the description of SMT-COMP organization and rules very interesting\par
and extremely important.\par
Many things about SMT-COMP are folklore and sometimes imprecise. It is\par
therefore very nice to read this article summarizing the organizational\par
issues and constraints of SMT-COMP,\par
making it very precise, so readers and future possible SMT-COMP attendants\par
know what to expect from SMT-COMP.\par
\par
\par
The main and most interesting part of the paper comes with the experimental\par
summary and evaluation of results from SMT-COMP 2014.\par
The results of the SMT-COMP are evaluated and summarized using different\par
metrics, including overall performance of tools and division-specific\par
summaries.\par
The experimental table are carefully edited and analyzed.\par
What I found also very interesting is the historical summary of tools\par
participation in previous and current edition of SMT-COMP.\par
I am not aware of the existence of such data, and it is nice to see the SMT\par
tools' trends.\par
The authors made very important contributions in the 2014 SMT-COMP setup:\par
- used the StarExec platform for running the competition, enabling thus time\par
performance comparisons for future edition of the competition.\par
Setting up the competition for  the StarExec platform required enormous\par
software engineering work, but paid off for future edition of SMT-COMP.\par
- strategies for benchmark selection (with no division-by-zero examples),\par
-  performance comparison on timing limits, providing quantitative answers\par
on how time limits effect tool performance.\par
The experiments give a very nice overview and explanation to many yet\par
unanswered questions (performance effect of timeout, time limits,\par
benchmarking).\par
\par
While the main focus of the paper is on SMT-COMP, the authors also manage to\par
give a mini crash course on SMT-Lib (v2), highlighting some of the main\par
syntactical constructs of SMT-Lib.\par
Hence, the paper can also be used as a starting point for people unfamiliar\par
with SMT-Lib.\par
\par
The paper is very well and carefully written. There is only one part where I\par
felt more details could have been given:\par
when describing benchmark selection for the competition, I did not fully\par
understood how\par
the solvers' integer seeds are used in the scrambling of benchmarks. It\par
would be good to give more details on this aspect of benchmark selection\par
I would also recommend that the authors insert the  experimental tables in\par
the main part of the paper, instead of giving these tables at the end of the\par
paper.\par
This would make the paper more reader-friendly, without the need of jumping\par
back-and-forth while reading the experimental part of the paper.\par
However, making these changes (and addressing the minor comments I list\par
below) requires minor revision, so I recommend accepting this paper after a\par
minor revision is done.\par
\par
Overall, I very much enjoyed reading this paper.\par
I strongly believe that the automated reasoning community needs such\par
articles, encouraging tool developments and competition on the one hand,\par
and providing precis summary of competition rules and results on the other\par
hand.\par
Setting up and running such tool competition is highly non-trivial, requires\par
lot of time and software engineering effort,\par
and also needs a good understanding of the logic for which the solvers are\par
designed.\par
I very much hope that future edition of SMT-COMP will also come with such a\par
nice report and summary as this paper does.\par
\par
*Minor comments*\par
- Section 1, page 1: "allow and disallow quantification" is sightly\par
misleading. Once the theory allows quantification, it becomes (in most\par
cases) undecidable, so one does not have decision procedures.\par
A similar comment goes also for page 2, when referring to "theories a kind\par
of typed first-order logic". The SMT theories are quantifier-free theories,\par
and for such theories decision procedures are used.\par
Quantifiers can be also used, however, unlike first-order provers, SMT\par
solvers use incomplete strategies and heuristics solving quantified\par
problems. This becomes clear later on page 3, still pages 1-2 are\par
misleading.\par
\par
- Section 4: "rules"->"competition rules"\par
________________________________________________________________________\par
\par
\par
}
 