\documentclass[twoside,11pt]{article}

\usepackage{jsat}
\usepackage{amsmath, amsthm, amssymb}
%\usepackage[pdf]{pstricks}
%\usepackage{pst-tree}

\usepackage{pifont}
\usepackage{comment}
\usepackage{multirow}

%\input epsf
\special{papersize=8.5in,11in}

\jsatheading{1}{2004}{25-30}
\ShortHeadings{The SMT Competition 2015--2018}
{Weber et al.}
\firstpageno{25}

\specialcomment{tjark}{\begingroup\footnotesize\color{red}}{\color{black}\endgroup}
\newcommand{\TODO}[0]{\textcolor{red}{TODO}}

\renewcommand{\mark}[0]{\ding{51}}

% TODO
\setcounter{topnumber}{2}
\setcounter{bottomnumber}{2}
\setcounter{totalnumber}{4}
\renewcommand{\topfraction}{0.85}
\renewcommand{\bottomfraction}{0.85}
\renewcommand{\textfraction}{0.15}
\renewcommand{\floatpagefraction}{0.7}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\title{The SMT Competition 2015--2018}

\author{%
  \name{Tjark Weber (competition chair, 2015--2018)}
  \email{tjark.weber@it.uu.se} \\
  \addr Uppsala University \\
  Uppsala \\
  Sweden
  \AND
  \name{Sylvain Conchon (co-organizer, 2015--2016)}
  \email{Sylvain.Conchon@lri.fr} \\
  \addr Affiliation \\
  City \\
  Country
  \AND
  \name{David D\'{e}harbe (co-organizer, 2015--2016)}
  \email{david@dimap.ufrn.br} \\
  \addr Federal University of Rio Grande do Norte \\
  Natal \\
  Brazil
  \AND
  \name{Matthias Heizmann (co-organizer, 2016--2018)}
  \email{heizmann@informatik.uni-freiburg.de} \\
  \addr Affiliation \\
  City \\
  Country
  \AND
  \name{Aina Niemetz (co-organizer, 2018)}
  \email{niemetz@cs.stanford.edu} \\
  \addr Affiliation \\
  City \\
  Country
  \AND
  \name{Giles Reger (co-organizer, 2017--2018)}
  \email{giles.reger@manchester.ac.uk} \\
  \addr Affiliation \\
  City \\
  Country}

\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{abstract}
  The International Satisfiability Modulo Theories Competition is an
  annual competition between Satisfiability Modulo Theories~(SMT)
  solvers.  The 2018 edition of the competition was part of the FLoC
  Olympic Games, which comprised 14 competitions in various areas of
  computational logic.  We report on the design and selected results
  of the SMT Competition during the last FLoC Olympiad, from 2015 to
  2018. \TODO: some notable observations/conclusions?
\end{abstract}

\keywords{SMT solver, SMT-COMP, SMT-LIB, Satisfiability Modulo Theories, competitions}

\published{September 2004}{October 2004}{November 2004}

%% Expected content according to the JSAT Call for Papers:

%% Full articles by competition organizers reporting on SAT 2018
%% competitions and evaluation. The articles should describe the
%% competition, its criteria, why it is interesting to the SAT
%% research community, execution environment used, analysis of the
%% results (including how they compare to previous instantiations, if
%% appropriate), and give a summary of the main technical
%% contributions to the field, as well as discussions on lessons
%% learned and suggestions for improvements for future competitions.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{tjark}
Changes 2015:
\begin{itemize}
\item All solvers will be run with $n=4$ cores available.  We will
  recognize both best sequential and best parallel performance in all
  main track divisions, using different CPU time limits.
\item The division scoring will take time spent on unsolved benchmarks
  into account.  Previous competitions only considered time spent on
  solved benchmarks.  However, the number of solved benchmarks still
  takes precedence over run-time.
\item In addition to recognizing the best solver in each division, we
  will recognize solvers that perform best according to
  competition-wide criteria, emphasizing breadth of supported logics.
  These criteria will be a refinement of the criteria used for the
  FLoC Olympic Games medals in~2014.
\item We plan to include new divisions for floating-point arithmetic.
  These should be considered experimental in~2015, and will not be
  included in the competition-wide ranking.
\item We plan to run solvers on all eligible benchmarks.  Previous
  competitions used a proper subset of eligible benchmarks, causing
  competition results to be affected by a random selection process.
\end{itemize}

Changes 2016:
\begin{itemize}
\item Benchmarks will use (a subset of) version~2.5 of the SMT-LIB
  language.  \emph{Rationale:} This is the latest version of the
  SMT-LIB language.  It was released on 2015-06-28, and is largely
  backwards-compatible to version~2.0, which was used for SMT-COMP
  2015.
\item Solver output may affect the score even when the solver does not
  terminate within the time limit.  (In 2015, main track solver output
  was ignored if the solver subsequently timed out.)  Solvers should
  take care not to accidentally produce output that contains
  \texttt{sat} or \texttt{unsat} even when they are killed.
  \emph{Rationale:} Users are likely to trust solver responses even
  when the solver continues to run for some time.
\item Divisions for floating-point arithmetic are no longer
  experimental, and will be considered competitive if the necessary
  requirements (see Section~\ref{sec:scoring}) are met.
  \emph{Rationale:} Floating-point divisions were experimental in
  2015.  By now, their definition is sufficiently stable, and they are
  supported by several solvers.
\item Division scores will be based on a weighted sum of scores for
  benchmark families.  \emph{Rationale:} For some years now, SMT-COMPs
  have had sufficient computational resources to evaluate all solver
  entrants on all eligible benchmarks.  As a side-effect, the
  weighting of benchmark families that was achieved in early SMT-COMPs
  through a careful selection of benchmarks (based on benchmark
  difficulty and category) was lost.  Since there are vast differences
  in size between benchmark families, we believe that a weighting that
  de-emphasizes large benchmark families (see
  Section~\ref{sec:division-scoring}) will lead to more meaningful
  competition results.
\item The unsat-core track that was introduced in SMT-COMP 2012, but
  discontinued in 2013-2015 (partly because of the competition's move
  to StarExec), is back.  It will be experimental in 2016; its results
  will be reported, but no official winners will be announced.
  \emph{Rationale:} Unsat cores are important in many applications of
  SMT solving.  The track was discontinued primarily for
  infrastructure and resource reasons.  We are pleased that we can
  finally offer it again, but we ask for your understanding in case of
  mishaps while we port the required tools.
\item Best industrial performance will no longer be recognized
  separately.  \emph{Rationale:} While there is agreement in the SMT
  community to emphasize problems that come from real applications,
  industrial performance largely coincided with overall performance in
  SMT-COMP 2015.  We consider the effort to determine and report it
  separately no longer justified.
\end{itemize}

Changes 2017:
\begin{itemize}
\item Submission of accompanying information for solvers is via a web
  form (rather than by email to the organizers).  \emph{Rationale:}
  Email submissions were often incomplete, leading to ambiguity and
  further inquiries.  The form provides a more structured way of
  submitting information.
\item Benchmarks will use (a subset of) version~2.6 of the SMT-LIB
  language.  \emph{Rationale:} This is the latest version of the
  SMT-LIB language.  It is largely backwards-compatible with earlier
  versions (in particular~2.0 and~2.5).
\item Benchmarks that use partial bit-vector functions, such as
  \texttt{bvudiv}, or underspecified floating-point functions, such as
  \texttt{fp.min}, will be eligible for the competition again.
  \emph{Rationale:} A conclusion on the semantics of these functions
  has finally been reached, and the status of affected SMT-LIB
  benchmarks has been updated accordingly.
\item Benchmarks with unknown status will be eligible for the
  competition's main track.  In 2016, solver performance on benchmarks
  with unknown status was evaluated but reported separately; in 2017,
  it will directly affect the competition results.  \emph{Rationale:}
  Using only benchmarks with known status (i.e., benchmarks that have
  been solved before) to determine the competition results unjustly
  favors imitation of existing solvers over true innovation.
\item In the competition-wide scoring, the (per-division) penalty for
  erroneous results is changed from the number of errors to a fixed
  constant (multiplied by the weight of the respective division).
  \emph{Rationale:} The weight already takes the number of benchmarks
  in the division into account; multiplying it by the number of errors
  overemphasizes large divisions.
\item The unsat-core track, which was re-introduced in 2016, is no
  longer experimental.  Its divisions will be considered competitive
  if the necessary requirements (see Section~\ref{sec:scoring}) are
  met.  \emph{Rationale:} Rules and tool support for the unsat-core
  track proved sufficiently stable in 2016.
\item The competition will feature experimental divisions for
  benchmarks that use algebraic datatypes.  \emph{Rationale:}
  Algebraic datatypes are specified in the draft SMT-LIB~2.6 standard,
  which is expected to become official in the near future, and are
  already supported by some solvers.  Benchmarks that use datatypes
  have recently been added to SMT-LIB.
\end{itemize}

Changes 2018:
\begin{itemize}
\item The time limit per solver/benchmark pair is anticipated to be at
  most 20 minutes in the main track, and at most 40 minutes in the
  other tracks.  \emph{Rationale:} Already in 2017, the time limit for
  the main track was reduced to 20 minutes (down from 40 minutes in
  earlier years) to cope with the inclusion of a large number of
  benchmarks with unknown status.
\item In the unsat-core track, the unsatisfiability check for the
  returned core will be based on a simple majority vote.  The case
  that a solver finds its own core satisfiable will no longer be
  treated specially.  \emph{Rationale:} This allows to simplify the
  post-processing of unsat-core track results considerably.
\item In 2017, the competition featured experimental divisions for
  benchmarks that use algebraic datatypes.  These divisions will be
  regular (non-experimental) in 2018. \emph{Rationale:} Algebraic
  datatypes are specified in the latest official release (Version~2.6)
  of the SMT-LIB standard.
\item The competition will feature an experimental divisions for
  benchmarks that use strings.  \emph{Rationale:} Corresponding
  theories and benchmarks are expected to be added to SMT-LIB in the
  near future.
\end{itemize}
\end{tjark}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}
\label{sec:introduction}

Satisfiability Modulo Theories (SMT) is a generalization of Boolean
satisfiability (SAT), the satisfiability decision problem for
propositional logic.  In place of Boolean variables, SMT formulas may
contain terms that are built from function and predicate symbols drawn
from a number of background theories.  Background theories, which are
motivated by application domains, include the theory of
arrays~\cite{TODO}, integer and real arithmetic~\cite{TODO},
bit-vectors~\cite{TODO}, and floating-point numbers~\cite{TODO} among
others.  For instance, the following is an SMT formula over the
combination of integer arithmetic and uninterpreted functions:
%
$$x \leq y \wedge y \leq x \wedge P (f(x) - f(y)) \wedge \neg P(0)$$
%
Software tools to determine the satisfiability of such formulas are
called SMT solvers.  With its rich input language, SMT has
applications in software engineering, optimization, and many other
areas~\cite{DeMoura:2011:SMT}.

Internally, many SMT solvers employ SAT solving techniques to deal
with the propositional structure of the formula, and combine these
with (semi-)decision procedures for the background theories that the
solver supports~\cite{TODO,TODO,TODO}.  Historically, SMT solvers have
focused on quantifier-free formulas and decidable combinations of
background theories.  Increasingly, however, SMT solvers also support
quantified formulas.  Hence there is overlap between SMT solving and
automated theorem proving for quantified Boolean formulas (QBF),
first-order logic, and even higher-order logic~\cite{TODO}.

The International Satisfiability Modulo Theories Competition
(SMT-COMP) is an annual competition between SMT solvers.  It was
instituted in~2005~\cite{BdMS05}, and is affiliated with the
International Workshop on Satisfiability Modulo Theories.  Solvers are
submitted to the competition by their developers, and pitted against
each other in a number of tracks and divisions.  The SMT Competition
was part of the Federated Logic Conference (FLoC) Olympic Games, which
comprised 14 competitions in various areas of computational logic, in
Vienna in~2014, and again in Oxford~(UK) in~2018.  It was last
described in a 2014 competition
report~\cite{DBLP:journals/jsat/CokDW14}.  In this paper, we report on
the design and selected results of the competition during the last
FLoC Olympiad, from 2015 to~2018.

\begin{tjark}
  WIP

  structure of the paper
\end{tjark}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{The Competition Goals and Organization}
\label{sec:goals}

The original goals of the competition were to spur adoption of the
common, community-designed SMT-LIB format (see
Section~\ref{sec:smtlib}), and to spark further advances in SMT,
especially for verification~\cite{BdMS05}.  These, together with
providing a useful yardstick of performance for users and developers
of SMT solvers, are still its main goals to date.  The competition has
been successful in establishing SMT-LIB as the de~facto standard
language for SMT solvers.  In recent years, the focus in this regard
has shifted towards promoting some of the newer extensions of the
SMT-LIB language, such as its floating-point theory or algebraic
datatypes~\cite{BarFT-RR-17}.

The competition is organized under the direction of the SMT Steering
Committee, which appoints the competition chair.  (This appointment
should happen ``within 30 days from the end of the current edition of
the SMT workshop''~\cite{smt-bylaws} but in practice has often
happened later.)  The competition chair then assembles a team of
organizers, and distributes the work for the next edition of the
competition.
%
The SMT Workshop includes a block of time to present the competitors
and results of the competition.  The workshop is affiliated with a
different conference each year: with CAV in 2015, with IJCAR in 2016,
again with CAV in 2017, and with FLoC in~2018.  Consequently, its date
varies by several weeks.  Competition results were presented on
July~19 in 2015, on July~2 in 2016, on July~23 in 2017, and on July~13
in~2018.

Important competition deadlines are determined by calculating
backwards from the date of the next SMT Workshop.  The competition's
computational workload should be completed approximately two weeks
before the workshop date.  This gives participants and other parties a
chance to scrutinize the data for errors, allows partial re-runs of
competition jobs if necessary, and gives the organizers time to
prepare the results presentation.  While making all data publicly
available well in advance of the official results presentation may
impact the suspense, it has on several occasions helped to uncover
serious problems with the competition tools or the StarExec framework
(see Section~\ref{sec:starexec}) that could have led to invalid
results otherwise.

It typically takes up to three weeks to run all competition jobs on
the StarExec cluster, so the final submission deadline for solvers
needs to be about five weeks before the workshop date.  The
competition imposes a separate deadline for first versions of solvers
about two weeks before the final submission deadline.  This earlier
deadline has proved useful for the organizers to obtain an accurate
estimate on the number of competing solvers, and to run preliminary
tests with some of the submitted solvers to identify potential issues.
Participants can still make changes to their solver until the final
deadline but are encouraged to use this period for bug-fixing only.

Around the initial solver deadline, the organizers also aim to publish
the latest version of the competition tools, and---in collaboration
with the SMT-LIB maintainers---to release a new version of the SMT
Library (see Section~\ref{sec:smtlib}).  SMT-LIB releases were made on
June~1 in 2015, on May~23 in 2016, on June~5 in 2017, and on May~20 in
2018.  Anyone may submit new benchmarks to the SMT Library, and
competitors are permitted to tune their solvers accordingly.
Therefore all benchmarks eligible for the competition must be released
at least some time before the final solver submission deadline.  To
give the SMT-LIB maintainers sufficient time for curation, the
deadline for new benchmark contributions has usually been about four
to six weeks before the initial solver deadline, typically in April or
early May.

The competition rules are revised each year, and a (near final) draft
version of the rules is made public on the competition web site around
mid-April.  The competition chair has ultimate responsibility for the
rules, but changes are often preceded by discussions among the
organizers or within the SMT community.  Such discussions are
initiated by a Call for Comments that is sent out to the SMT-COMP
mailing list~\cite{smtcomp-mailinglist} typically in February or early
March.  Publicly, this call is the first harbinger of a new edition of
the SMT Competition.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{The SMT-LIB Language and Library}
\label{sec:smtlib}

All problems used in the SMT Competition come from the SMT Library
(SMT-LIB).  This is a large collection of benchmark problems from
various sources.  The SMT Library is available from a public
repository~\cite{smtlib-repository}, and each new release is mirrored
on StarExec (see Section~\ref{sec:starexec}), the cluster on which the
competition runs.  Users of SMT solvers are encouraged to submit new
and interesting benchmarks to the library, which has grown from 1,352
benchmarks in 2005~\cite{BdMS05} to 347,011 benchmarks in 2018.

Benchmarks in the SMT Library are written in the SMT-LIB
language~\cite{BarFT-RR-17}, a text-based format that defines the
syntax and semantics of solver input and output.  This includes a
language for terms and formulas, and a command language for
interacting with SMT solvers.  For instance,
Figure~\ref{fig:smtlib-example} shows the example formula from
Section~\ref{sec:introduction} as a benchmark in SMT-LIB syntax.

\begin{figure}
\begin{verbatim}
(set-info :smt-lib-version 2.6)
(set-logic QF_UFLIA)
(set-info :status unsat)
(declare-fun x () Int)
(declare-fun y () Int)
(declare-fun f (Int) Int)
(declare-fun P (Int) Bool)
(assert (and (<= x y) (<= y x) (P (- (f x) (f y))) (not (P 0))))
(check-sat)
(exit)
\end{verbatim}
\caption{A benchmark problem in SMT-LIB syntax.}
\label{fig:smtlib-example}
\end{figure}

The SMT Library classifies benchmarks according to their logic, that
is, according to the specific combination of background theories that
the benchmark uses.  Logic names are composed of letter groups that
refer to these background theories: {\tt A} or {\tt AX} for the theory
of arrays, {\tt BV} for the theory of bit-vectors, {\tt DT} for
algebraic datatypes, {\tt FP} for the floating-point theory, {\tt IA}
for integer arithmetic, {\tt RA} for real arithmetic, {\tt IRA} for
mixed integer and real arithmetic, {\tt S} for the theory of strings,
{\tt UF} for uninterpreted functions (i.e., free symbols).
Additionally, logics may impose further syntactic restrictions, such
as the absence of quantifiers ({\tt QF}), the restriction to
difference logic over the integers ({\tt IDL}) or reals ({\tt RDL}),
or to the (non-)linear fragment of arithmetic ({\tt L} or {\tt N},
respectively, before {\tt IA}, {\tt RA} or {\tt IRA}).  For instance,
the logic {\tt QF\_UFLIA} contains quantifier-free formulas with
uninterpreted functions over linear integer arithmetic.  These
restrictions are typically motivated by the existence of efficient
decision procedures for certain syntactic fragments.

Additionally, the SMT Library distinguishes between incremental and
non-incremental benchmarks.  Non-incremental benchmarks contain a
single {\tt check-sat} command, which instructs the SMT solver to
determine the satisfiability of the benchmark.  (Valid solver
responses are {\tt sat}, {\tt unsat}, and {\tt unknown}.)  Incremental
benchmarks exercise additional solver features, such as the ability to
retract assertions, and typically contain multiple {\tt check-sat}
commands.  These benchmarks originate from applications such as
bounded model checking~\cite{TODO}, where SMT solvers interact with
other tools in a feedback loop.  The 2018 release of the SMT Library
contained 336,844 non-incremental benchmarks in 51 logics, as well
as~10,167 incremental benchmarks (with a total of nearly 32 million
{\tt check-sat} commands) in 26 logics.  Table~\ref{table:smtlib}
presents further data on the size of the SMT Library since~2015.
While the library usually grows from one year to the next due to the
inclusion of new benchmarks and logics, continuous curation efforts
may also cause benchmarks to be removed or reclassified on occasion,
thereby causing individual logics or even the entire library to
shrink.

\begin{table}
  \caption{Number of benchmarks and logics in the SMT Library}
  \label{table:smtlib}
  \centering
  \begin{tabular}{|r@{\ \ }l|r|r|r|r|}
    \hline
                               & & \multicolumn{1}{c|}{2015} & \multicolumn{1}{c|}{2016} & \multicolumn{1}{c|}{2017} & \multicolumn{1}{c|}{2018} \\
    \hline
    \multirow{2}{*}{Non-incremental $\{$} & benchmarks & 196,375 & 196,114 & 258,079 & 336,844 \\
                                          & logics     &      40 &      40 &      49 &      51 \\
    \multirow{2}{*}{Incremental $\{$}     & benchmarks &  10,019 &  10,024 &   6,262 &  10,167 \\
                                          & logics     &      15 &      15 &      22 &      26 \\
    \hline
  \end{tabular}  
\end{table}

Benchmarks in the SMT Library are annotated with additional
information, such as the source of the benchmark and its status (i.e.,
whether the benchmark is satisfiable or unsatisfiable) if this is
known.  (For incremental benchmarks, each {\tt check-sat} command has
a separate status annotation.)  To prevent solvers from simply looking
up the benchmark's status in the SMT Library, benchmarks in the
competition are lightly scrambled before they are presented to
solvers~\cite{DBLP:conf/cade/Weber16}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Competition Divisions and Benchmarks}
\label{sec:benchmarks}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{StarExec}
\label{sec:starexec}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Participants}
\label{sec:participants}

Participants must upload their solvers to StarExec to enter them into
the competition, and additionally submit information about each
competing solver---such as the tracks and divisions into which the
solver is being entered---to the competition organizers.  Until 2016,
this information was collected by email.  Since 2017, a web form is
being used for this purpose.  This has greatly reduced the number of
incomplete or ambiguous submissions.

Table~\ref{table:participants} lists the solvers that participated
between~2015 and~2018, together with their submitter(s) and
institutional affiliations (which may have changed over time).  Note
that most solvers were developed by a group of people; the table does
not list all contributors but only submitters.
%
Participants are also encouraged to submit a short system description
of their solver, including a list of its authors and an explanation of
the SMT solving approach used.  System descriptions are published on
the SMT-COMP website~\cite{smtcomp-web}.  They were submitted for
about 60\,\% of all solver versions since 2015.

\begin{table}
  \caption{Participants (2015--2018)}
  \label{table:participants}
  \centering
  \begin{tabular}{|l|p{13cm}|} %TODO: fix width
    \hline
    Solver & Submitters (Affiliations)\\
    \hline
    ABC [\TODO]       & Valera Balabanov
                        (Mentor Graphics)\\
    Alt-Ergo          & Albin Coquereau
                        (CNRS and University of Paris-Sud)\\
    AProVE            & Carsten Fuhs
                        (University College London; Birkbeck, University of London)\\
    Boolector         & Mathias Preiner
                        (Johannes Kepler University)\\
    COLIBRI           & Fran\c{c}ois Bobot
                        (French Alternative Energies and Atomic Energy Commission)\\
    Ctrl-Ergo         & Mohamed Iguernlala
                        (CNRS and University of Paris-Sud)\\
    CVC3              & Kshitij Bansal
                        (New York University)\\
    CVC4              & Kshitij Bansal, Clark Barrett, Andres N\"otzli, Duligur Ibeling
                        (New York University; Stanford University)\\
    MapleSTP          & Jimmy Liang
                        (University of Waterloo)\\
    MinkeyRink        & Trevor Hansen
                        (University of Melbourne)\\
    OpenSMT2          & Matteo Marescotti, Antti Hyv\"arinen
                        (University of Lugano)\\
    ProB              & Sebastian Krings
                        (University of D\"usseldorf)\\
    Q3B               & Martin Jon\'a\v{s}
                        (Masaryk University)\\
    raSAT             & Tung Vu Xuan
                        (Japan Advanced Institute of Science and Technology)\\
    Redlog            & Haniel Barbosa
                        (Inria Nancy)\\
    SMT-RAT           & Florian Corzilius, Gereon Kremer
                        (RWTH Aachen University)\\
    SMTInterpol       & J\"urgen Christ
                        (University of Freiburg)\\
    SPASS-SATT        & Martin Bromberger
                        (Max Planck Institute for Informatics)\\
    STP               & Mate Soos, Trevor Hansen, Norbert Manthey
                        (National University of Singapore; University of Melbourne; Amazon Web Services)\\
    toysmt            & Masahiro Sakai
                        (Toshiba Corporation)\\
    Vampire           & Giles Reger
                        (University of Manchester)\\
    veriT             & David D\'eharbe, Haniel Barbosa, Hans-J\"org Schurr
                        (Federal University of Rio Grande do Norte; Inria Nancy)\\
    XSat              & Martin Velez
                        (University of California, Davis)\\
    Yices2            & Bruno Dutertre, Dejan Jovanovi\'c
                        (SRI International)\\
    Z3                & Christoph Wintersteiger, Aleksandar Zelji\'c
                        (Microsoft Research; Uppsala University)\\
    \hline
  \end{tabular}  
\end{table}

Many of these solvers were submitted in multiple years.  Additionally,
to promote a wide comparison between tools, developers are allowed to
submit multiple versions of a solver, e.g., for different competition
tracks or with differently tuned internal components.
Table~\ref{table:participants-history} shows in more detail how many
versions of each solver were submitted in each year.

\begin{table}
  \caption{Number of versions submitted for each solver (by year).
    Numbers in square brackets indicate solver versions that were
    participating hors concours.}
  \label{table:participants-history}
  \centering
  \begin{tabular}{|l|r@{\,\,}r|r@{\,\,}r|r@{\,\,}r|r@{\,\,}r|}
    \hline
    Solver & \multicolumn{2}{c|}{2015} & \multicolumn{2}{c|}{2016} & \multicolumn{2}{c|}{2017} & \multicolumn{2}{c|}{2018} \\
    \hline
    ABC               &    &      &  2 &      &    &      &    &      \\
    Alt-Ergo          &    &      &    &      &    &      &  1 &      \\
    AProVE            &  1 &      &  1 &      &  1 &      &  1 &      \\
    Boolector         &  3 & [+1] &  2 &      &  2 &      &  2 &      \\
    COLIBRI           &    &      &    &      &  1 &      &  1 &      \\
    Ctrl-Ergo         &    &      &    &      &    &      &  1 &      \\
    CVC3              &  2 &      &    &      &    &      &    &      \\
    CVC4              &  4 &      &  1 & [+1] &  3 &      &  3 & [+1] \\
    MapleSTP          &    &      &  4 &      &    &      &    &      \\
    {[}MathSAT{]}     &    & [+2] &    & [+3] &    & [+3] &    & [+3] \\
    MinkeyRink        &    &      &  1 &      &  1 &      &  2 &      \\
    OpenSMT2          &  2 &      &  1 &      &  1 &      &    & [+1] \\
    ProB              &    &      &  1 &      &    &      &    &      \\
    Q3B               &    &      &  1 &      &  1 &      &  1 &      \\
    raSAT             &  1 &      &  2 &      &    &      &    &      \\
    Redlog            &    &      &    &      &  1 &      &    &      \\
    SMT-RAT           &  2 &      &  1 &      &  1 &      &  2 &      \\
    SMTInterpol       &  1 &      &  1 &      &  1 &      &  1 &      \\
    SPASS-SATT        &    &      &    &      &    &      &  1 &      \\
    STP               &  4 &      &  8 &      &  2 &      &  3 &      \\
    toysmt            &    &      &  1 &      &    &      &    &      \\
    Vampire           &    &      &  2 &      &  1 &      &  1 &      \\
    veriT             &  1 &      &  1 &      &  3 &      &  2 &      \\
    XSat              &    &      &    &      &  1 &      &    &      \\
    Yices2            &  3 &      &  2 &      &  2 &      &  3 &      \\
    Z3                &  2 & [+1] &    & [+1] &    & [+1] &    & [+1] \\
    \hline
    Total             & 26 & [+4] & 32 & [+5] & 22 & [+4] & 25 & [+6] \\
    \hline
  \end{tabular}
\end{table}

In addition to the solvers that were submitted to the competition by
their respective developers, the organizers included the most recent
stable versions of MathSAT and Z3 for comparison purposes.  Both
solvers are strong tools, but---except for two experimental versions
of Z3 in 2015---their development teams chose not to prepare
competition versions.  These solvers were therefore participating hors
concours.  Moreover, in 2015 a bug was discovered in the application
track version of Boolector and a fixed version was submitted after the
deadline; in 2016, the CVC4 team did not enter their solver into the
application track; and in 2018, an experimental version of CVC4 as
well as the OpenSMT2 solver were submitted after the deadline.  These
solvers were also participating hors concours.  In result tables, they
are listed with their name in square brackets (e.g., [MathSAT]).

Table~\ref{table:participation-by-track} summarizes these figures and
shows how many solver versions were submitted to each track of the
competition.  Note that solver versions may be entered into multiple
tracks.  Therefore, the total number of solver versions for each year
is typically less than the sum over all tracks.

By number of solver versions submitted, the competitions in 2015--2018
were the four largest in the history of SMT-COMP.  On average, 26
solver versions were submitted each year since 2015.  In contrast, the
competitions from 2005--2014 only received an average of 12
submissions~\cite{DBLP:journals/jsat/CokDW14}.

\begin{table}
  \caption{Participation by track and year.  Numbers in square
    brackets indicate solver versions that were participating hors
    concours.}
  \label{table:participation-by-track}
  \centering
  \begin{tabular}{|l|r@{\,\,}r|r@{\,\,}r|r@{\,\,}r|r@{\,\,}r|}
    \hline
                      & \multicolumn{2}{c|}{2015} & \multicolumn{2}{c|}{2016} & \multicolumn{2}{c|}{2017} & \multicolumn{2}{c|}{2018} \\
    \hline
    Main track        & 21 &                [+2] & 25 & [+2] & 19 & [+2] & 20 & [+4] \\
    Application track & 10 &                [+3] &  8 & [+3] &  4 & [+2] &  4 & [+2] \\
    Unsat-core track  & \multicolumn{2}{c|}{---} &  1 & [+4] &  2 & [+2] &  3 & [+2] \\
    \hline
    Total             & 26 &                [+4] & 32 & [+5] & 22 & [+4] & 25 & [+6] \\
    \hline
  \end{tabular}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Competition Procedure}
\label{sec:procedure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Results}
\label{sec:results}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{FLoC Olympic Games}
\label{sec:floc}

In 2014, the SMT Competition was part of the FLoC Olympic Games.
These gave the competition increased visibility and sponsored three
Kurt G\"odel medals.  After substantial internal debate, the 2014
competition organizers decided to award these to the winner of the
main track's QF\_BV division (which has historically been the logic
with the largest number of solver submissions and job pairs), as well
as to the two best main-track solvers according to a competition-wide
ranking that had been instituted especially for this
purpose~\cite{DBLP:journals/jsat/CokDW14}.  This ranking emphasizes
breadth of solver participation---a solver participating in many
logics need not be the best in any one of them to rank highly.

The competition-wide ranking for the main track was retained and
refined after~2014.  A contentious issue with this ranking has been
the scoring of incorrect solver responses.  In the per-division
rankings, an incorrect response causes a solver to be ranked behind
all correct solvers~\cite{rules18}.  But the competition's main track
consists of many independent divisions.  To balance the strong
interest in correct solvers against the risk of stifling innovation,
the competition-wide ranking was revised in~2017 to use a fixed
numeric penalty for each division with incorrect responses.  The value
of this penalty is currently set so that entering a (possibly buggy)
solver that can solve all benchmarks into a division has a positive
expected score if the probability of a soundness bug in the solver is
below~$20\,\%$.

The 2018 edition of the competition was again part of the FLoC Olympic
Games, which sponsored seven Kurt G\"odel medals.  The competition
organizers awarded these to the following solvers:
\begin{itemize}
\item The best three main-track solvers according to the
  competition-wide ranking (first place: CVC4, second place:
  Yices-2.6.0, third place: SMTInterpol);
\item the solver(s) that won the QF\_BV division in the main track
  (sequential ranking: Boolector, parallel ranking: Minkeyrink-MT);
\item the solver that won the most competitive divisions in the
  application track (Yices-2.6.0);
\item the solver that won the most competitive divisions in the
  unsat-core track (Yices-2.6.0).
\end{itemize}

In comparison to the competition-wide ranking for the main track, the
metric used for the application and unsat-core track---simply counting
the number of competitive divisions that a solver has won---was rather
unsophisticated.  Future editions of the competition should perhaps
refine this to use a ranking scheme that is more similar to the
main-track ranking, by weighing divisions according to their relative
importance, and also by rewarding good performance from solvers other
than division winners.  A difficulty in adapting the main-track
ranking for the other tracks is that each main-track benchmark is
worth one point (before weighing) if solved correctly, while
benchmarks in the application and unsat-core track are worth widely
varying numbers of points.  Moreover, in the unsat-core track, the
size of the smallest unsatisfiable core---and hence the number of
points attainable on any given benchmark---may not be known precisely.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Conclusions}
\label{sec:conclusions}

\TODO

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bibliographystyle{plainbv}
\bibliography{SMT-COMP}

\end{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Local Variables:
% ispell-local-dictionary: "american"
% mode: LaTeX
% mode: flyspell
% LocalWords: concours hors satisfiability SMT Tjark
% End:
