Workflow for problem selection

Follow these steps per track.

1. 
Get new benchmarks. Hopefully these are nicely stored in a directory structure such as benchmarks-pending/DONE/2019/non-incremental

If so, run (for example)

sh prepare_new_single.sh <path-to-new-benchmarks>

to put the names of these in an appropriate csv file. Otherwise, see selection.py on the format required.

From 2019 onwards these will appear in the repository under <year>/prep/

2.
Edit select.py if you need to. It has some hard-coded values based on the competition rules document. Read the comments to see if you want to update these.

3.
As well as a CSV file containing new benchmarks you should also use a CSV file containing the old benchmarks from the previous year's competition. This will be used to identify old benchmarks and to filter benchmark selection if appropriate. These CSV files are archived in the repository under <year>/csv. If you want to use a different set of base benchmarks then you can use any StarExec results CSV file. You can also supply an empty CSV file.

You will also need a seed value for the pseudo-random selection.

With these inputs you should run:

python selection.py --new_csv <new csv file> --old_csv <old csv file> --seed <seed> --out <out file> 

to print the names of the selected problems into the supplied out file. These can be used with the space preperation scripts to prepare a competition space. 

If you want to filter the problems as set out in the rule document for the single query track you should also add '--filter on'


